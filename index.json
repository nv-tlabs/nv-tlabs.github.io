[{"authors":["sanjafidler"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a008223156da402f6f035b4e37e1bc13","permalink":"/author/sanja-fidler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sanja-fidler/","section":"authors","summary":"","tags":null,"title":"Sanja Fidler","type":"authors"},{"authors":["hassanabualhaija"],"categories":null,"content":"I’m a Research Engineer at NVIDIA Toronto AI Lab based in Heidelberg, Germany. I did my PhD in University of Heidelberg with Prof. Carsten Rother jointly with Prof. Andreas Geiger. My research passion is at the border between Machine Learning and Graphics, especially in building ML tools that can make 3D creation and rendering more accessible and intuitive.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1defd4ca148c0a4c12048100b5918a16","permalink":"/author/hassan-abu-alhaija/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hassan-abu-alhaija/","section":"authors","summary":"I’m a Research Engineer at NVIDIA Toronto AI Lab based in Heidelberg, Germany. I did my PhD in University of Heidelberg with Prof. Carsten Rother jointly with Prof. Andreas Geiger. My research passion is at the border between Machine Learning and Graphics, especially in building ML tools that can make 3D creation and rendering more accessible and intuitive.","tags":null,"title":"Hassan Abu Alhaija","type":"authors"},{"authors":["bradleybrown"],"categories":null,"content":"I\u0026rsquo;m an undergraduate student at the University of Waterloo, studying software engineering with a joint major in combinatorics and optimization. At NVIDIA, my research focuses on generative modelling. In particular, extensions to 3D and interactive generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"37382a17d7aef3d50dedb35eb76700c8","permalink":"/author/bradley-brown/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bradley-brown/","section":"authors","summary":"I\u0026rsquo;m an undergraduate student at the University of Waterloo, studying software engineering with a joint major in combinatorics and optimization. At NVIDIA, my research focuses on generative modelling. In particular, extensions to 3D and interactive generation.","tags":null,"title":"Bradley Brown","type":"authors"},{"authors":["davidacunamarrero"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"052a025ce9438f4d0241f93c9d69c255","permalink":"/author/david-acuna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-acuna/","section":"authors","summary":"","tags":null,"title":"David Acuna","type":"authors"},{"authors":["matanatzmon"],"categories":null,"content":"I am a PhD student at the Weizmann Institute of Science. My research focuses on devising 3D deep learning methods, mostly interested in learning with weak supervision, and 3D generative models.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"053495656778fd3f66b2df0c6e6c020d","permalink":"/author/matan-atzmon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matan-atzmon/","section":"authors","summary":"I am a PhD student at the Weizmann Institute of Science. My research focuses on devising 3D deep learning methods, mostly interested in learning with weak supervision, and 3D generative models.","tags":null,"title":"Matan Atzmon","type":"authors"},{"authors":["tianshicao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"22fab2016a26de835b2c60c65dd3c6ce","permalink":"/author/tianshi-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianshi-cao/","section":"authors","summary":"","tags":null,"title":"Tianshi Cao","type":"authors"},{"authors":["wenzhengcheng"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d4aab82c8b2f97308fd9b03061baa62","permalink":"/author/wenzheng-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenzheng-chen/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Wenzheng Chen","type":"authors"},{"authors":["riccardodelutio"],"categories":null,"content":"I am a PhD student at ETH Zurich in the EcoVision Lab, Photogrammetry and Remote Sensing Group under the supervision of Prof. Jan Wegner and Prof. Konrad Schindler. My research interests are super-resolution, depth upsampling/completion, fine-grained classification, biodiversity monitoring, and environmental data.\nAfter completing my master studies, I joined the Machine Learning and Optimisation Lab at EPFL to work on a project which aim was to create an algorithm that could help clinicians better predict the causes of pediatric fever in Africa. Following this experience, I decided to continue to focus my research on using Machine Learning to tackle important global issues. My main work at EcoVision is on estimating biodiversity in Switzerland using satellite images.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8fe28a806af94a27e19cea6b301e698f","permalink":"/author/riccardo-de-lutio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/riccardo-de-lutio/","section":"authors","summary":"I am a PhD student at ETH Zurich in the EcoVision Lab, Photogrammetry and Remote Sensing Group under the supervision of Prof. Jan Wegner and Prof. Konrad Schindler. My research interests are super-resolution, depth upsampling/completion, fine-grained classification, biodiversity monitoring, and environmental data.","tags":null,"title":"Riccardo de Lutio","type":"authors"},{"authors":["nishkritdesai"],"categories":null,"content":"I am an undergrad at the University of Toronto studying Engineering Science. I\u0026rsquo;m currently a Research Engineer Intern working on generative models for textures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"405213799b30f1394f2ca79ed05a3524","permalink":"/author/nishkrit-desai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nishkrit-desai/","section":"authors","summary":"I am an undergrad at the University of Toronto studying Engineering Science. I\u0026rsquo;m currently a Research Engineer Intern working on generative models for textures.","tags":null,"title":"Nishkrit Desai","type":"authors"},{"authors":["clementfujitsang"],"categories":null,"content":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c2429b6ac12c5dc84df0223e7517c65","permalink":"/author/clement-fuji-tsang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/clement-fuji-tsang/","section":"authors","summary":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.","tags":null,"title":"Clement Fuji Tsang","type":"authors"},{"authors":["jungao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4290fc14b9975f84c3d7f082f170356","permalink":"/author/jun-gao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jun-gao/","section":"authors","summary":"","tags":null,"title":"Jun Gao","type":"authors"},{"authors":["zangojcic"],"categories":null,"content":"I am a research scientist at NVIDIA. I am broadly interested in general 3D vision problems. During my Ph.D. my research has mostly focused on incorporating a local rigidity inductive bias into models for point cloud registration and scene flow estimation.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e07d7147e171cebad490a9010f911403","permalink":"/author/zan-gojcic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zan-gojcic/","section":"authors","summary":"I am a research scientist at NVIDIA. I am broadly interested in general 3D vision problems. During my Ph.D. my research has mostly focused on incorporating a local rigidity inductive bias into models for point cloud registration and scene flow estimation.","tags":null,"title":"Zan Gojcic","type":"authors"},{"authors":["timdockhorn"],"categories":null,"content":"I am a PhD student at the University of Waterloo working on a variety of topics in machine learning. In my internship I am working on improving large generative models by using fundamental ideas from physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ebc05bd7317ed4f991addb6d713f4790","permalink":"/author/tim-dockhorn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tim-dockhorn/","section":"authors","summary":"I am a PhD student at the University of Waterloo working on a variety of topics in machine learning. In my internship I am working on improving large generative models by using fundamental ideas from physics.","tags":null,"title":"Tim Dockhorn","type":"authors"},{"authors":["amlankar"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cc1152b762fdb5eb7959a8087610fe02","permalink":"/author/amlan-kar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amlan-kar/","section":"authors","summary":"","tags":null,"title":"Amlan Kar","type":"authors"},{"authors":["mohamedhassan"],"categories":null,"content":"I am a Ph.D. student at Max Planck Institute for Intelligent Systems, where I work on computer vision, computer graphics and machine learning. I am advised by Michael Black. I am interested in reconstructing, analyzing, and generating Human-Scene Interaction.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"311ad85f1f2e978f23c2b8c103ea9cf3","permalink":"/author/mohamed-hassan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mohamed-hassan/","section":"authors","summary":"I am a Ph.D. student at Max Planck Institute for Intelligent Systems, where I work on computer vision, computer graphics and machine learning. I am advised by Michael Black. I am interested in reconstructing, analyzing, and generating Human-Scene Interaction.","tags":null,"title":"Mohamed Hassan","type":"authors"},{"authors":["samehkhamis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"732c7bd8ae9bcb1d929f34f6620da459","permalink":"/author/sameh-khamis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sameh-khamis/","section":"authors","summary":"","tags":null,"title":"Sameh Khamis","type":"authors"},{"authors":["anitahu"],"categories":null,"content":"I\u0026rsquo;m a research engineering intern and undergrad at the University of Waterloo. At Nvidia, I am working on bring research works to production as extensions in the AI Playground\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9b00c7e40ac664fdd4f3f2104d69e1b7","permalink":"/author/anita-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anita-hu/","section":"authors","summary":"I\u0026rsquo;m a research engineering intern and undergrad at the University of Waterloo. At Nvidia, I am working on bring research works to production as extensions in the AI Playground","tags":null,"title":"Anita Hu","type":"authors"},{"authors":["jiahuihuang"],"categories":null,"content":"Jiahui Huang is a fourth-year Ph.D. candidate of the Graphics and Geometric Computing Group in BRCist / CS Department at Tsinghua University, China, advised by Prof. Shi-Min Hu. His research focuses on dynamic perception and 3D reconstruction using SLAM and deep learning.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"53da64d87d93008ffa5e37ff1949df79","permalink":"/author/jiahui-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiahui-huang/","section":"authors","summary":"Jiahui Huang is a fourth-year Ph.D. candidate of the Graphics and Geometric Computing Group in BRCist / CS Department at Tsinghua University, China, advised by Prof. Shi-Min Hu. His research focuses on dynamic perception and 3D reconstruction using SLAM and deep learning.","tags":null,"title":"Jiahui Huang","type":"authors"},{"authors":["shengyuhuang"],"categories":null,"content":"I am a second-year PhD student in Photogrammetry and Remote Sensing Lab of ETH Zürich, jointly supervised by Prof. Dr. Konrad Schindler and Prof. Dr. Andreas Wieser. Currently, I am a research scientist intern at NVIDIA Toronto AI Lab with Prof. Dr. Sanja Fidler, working on neural simulation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84a2d3644c56161190103fe9cd11e79c","permalink":"/author/shengyu-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shengyu-huang/","section":"authors","summary":"I am a second-year PhD student in Photogrammetry and Remote Sensing Lab of ETH Zürich, jointly supervised by Prof. Dr. Konrad Schindler and Prof. Dr. Andreas Wieser. Currently, I am a research scientist intern at NVIDIA Toronto AI Lab with Prof.","tags":null,"title":"Shengyu Huang","type":"authors"},{"authors":["seungwookkim"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"594015d730259c3a92f033305a532e2f","permalink":"/author/seung-wook-kim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/seung-wook-kim/","section":"authors","summary":"","tags":null,"title":"Seung Wook Kim","type":"authors"},{"authors":["karstenkreis"],"categories":null,"content":"I am a Senior Research Scientist at NVIDIA’s Toronto AI Lab. I am trained as a physicist and completed my master’s in quantum information theory. For my Ph.D. in computational and statistical physics, I developed multiscale models and sampling algorithms for molecular dynamics simulations of complex chemical and biological systems. After I finished my Ph.D., I switched to deep learning. Before joining NVIDIA, I worked on deep generative modeling at D-Wave Systems, a quantum computation company, and I co-founded Variational AI, a startup focusing on generative modeling for drug discovery.\nI have always been excited by developing mathematical frameworks and algorithmic and data-driven approaches to simulate and model our physical world and to synthesize novel but realistic data from scratch. Currently, my primary research interests revolve around deep generative models. I am interested both in fundamental algorithm development and in applying these models on relevant problems in areas such as representation learning, computer vision, graphics and digital artistry. I am also broadly interested in research that takes inspirations from physics to improve machine learning techniques as well as in applying state-of-the-art deep learning methods to problems in the natural sciences.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a216c907a09258df8f398909408d130a","permalink":"/author/karsten-kreis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/karsten-kreis/","section":"authors","summary":"I am a Senior Research Scientist at NVIDIA’s Toronto AI Lab. I am trained as a physicist and completed my master’s in quantum information theory. For my Ph.D. in computational and statistical physics, I developed multiscale models and sampling algorithms for molecular dynamics simulations of complex chemical and biological systems.","tags":null,"title":"Karsten Kreis","type":"authors"},{"authors":["marclaw"],"categories":null,"content":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks. More specifically, my domains of interest are: distance metric learning, complex representations including hierarchies and graphs, convex and non-convex optimization, optimization on manifolds, information geometry, structured output prediction, generalization with limited supervision.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f02ed7ceb48db52a1c5142bb4607614b","permalink":"/author/marc-law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marc-law/","section":"authors","summary":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks.","tags":null,"title":"Marc Law","type":"authors"},{"authors":["michaelli"],"categories":null,"content":"I am a research engineer intern working on Kaolin. Broadly, I am interested in applying research findings to real world problems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d283b77e1295e97ba63f721cba4a4728","permalink":"/author/michael-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-li/","section":"authors","summary":"I am a research engineer intern working on Kaolin. Broadly, I am interested in applying research findings to real world problems.","tags":null,"title":"Michael Li","type":"authors"},{"authors":["derekliu"],"categories":null,"content":"Hsueh-Ti Derek Liu is a Ph.D. student at the University of Toronto supervised by Alec Jacobson. Derek received his B.S.E. at National Taiwan University, and M.S. at Carnegie Mellon University advised by Keenan Crane and Levent Burak Kara. Derek\u0026rsquo;s research focuses on digital geometry processing. He aims at developing easy-to-use tools for 3D content creation and algorithms for processing geometric data at scale. Derek\u0026rsquo;s research is partially supported by Adobe Research Fellowship and the Mary H. Beatty Fellowship. Recently, he is doing an internship at NVIDIA mentored by Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57eb045cf7de2b7c6ffb7240f52d8cb6","permalink":"/author/derek-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/derek-liu/","section":"authors","summary":"Hsueh-Ti Derek Liu is a Ph.D. student at the University of Toronto supervised by Alec Jacobson. Derek received his B.S.E. at National Taiwan University, and M.S. at Carnegie Mellon University advised by Keenan Crane and Levent Burak Kara.","tags":null,"title":"Derek Liu","type":"authors"},{"authors":["jonathanlorraine"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA and a Ph.D. candidate in machine learning at the University of Toronto. My research focuses on hyperparameter optimization, learning in games, and - more generally - nested optimization. At NVIDIA, I’m working on extending methods for learning when our data is neural networks.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8f975d3e91292e1a6137ba3368072e77","permalink":"/author/jonathan-lorraine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonathan-lorraine/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA and a Ph.D. candidate in machine learning at the University of Toronto. My research focuses on hyperparameter optimization, learning in games, and - more generally - nested optimization.","tags":null,"title":"Jonathan Lorraine","type":"authors"},{"authors":["phongnguyenha"],"categories":null,"content":"I am a PhD student at the University of Oulu, Finland. I\u0026rsquo;m interested in the topic of 3D reconstruction, novel view synthesis and neural rendering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3aa4960ec26f0e966a652a86f5c1be0e","permalink":"/author/phong-nguyen-ha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/phong-nguyen-ha/","section":"authors","summary":"I am a PhD student at the University of Oulu, Finland. I\u0026rsquo;m interested in the topic of 3D reconstruction, novel view synthesis and neural rendering.","tags":null,"title":"Phong Nguyen-Ha","type":"authors"},{"authors":["daiqingli"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ee0d6dbbf827a332ae71729722511b7a","permalink":"/author/daiqing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daiqing-li/","section":"authors","summary":"","tags":null,"title":"Daiqing Li","type":"authors"},{"authors":["huanling"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eb3fc86e8c4373b23194a1d7a96d5481","permalink":"/author/huan-ling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huan-ling/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Huan Ling","type":"authors"},{"authors":["virajprabhu"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA and Ph.D. student at Georgia Tech advised by Prof. Judy Hoffman. My research interests are in developing data-efficient and reliable computer vision systems that can be deployed in the real world. Specifically, I am interested in designing algorithms for reducing (via few-shot and active learning), and reusing (via domain adaptation) human supervision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1baff9caeb45e1ab9663e75b4f6f612c","permalink":"/author/viraj-prabhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/viraj-prabhu/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA and Ph.D. student at Georgia Tech advised by Prof. Judy Hoffman. My research interests are in developing data-efficient and reliable computer vision systems that can be deployed in the real world.","tags":null,"title":"Viraj Prabhu","type":"authors"},{"authors":["orlitany"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"446ebc3097a2014a6dac95a5dcfe857c","permalink":"/author/or-litany/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/or-litany/","section":"authors","summary":"","tags":null,"title":"Or Litany","type":"authors"},{"authors":["jameslucas"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD candidate at the University of Toronto. My research broadly works towards a better understanding of deep learning and the optimization of neural networks. At NVIDIA, I\u0026rsquo;m developing methods to improve deep learning models in data-scarce settings and working with generative models of 3D data.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84b15291b5248c5e6cdeddee7ae1a3e6","permalink":"/author/james-lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/james-lucas/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD candidate at the University of Toronto. My research broadly works towards a better understanding of deep learning and the optimization of neural networks.","tags":null,"title":"James Lucas","type":"authors"},{"authors":["davisrempe"],"categories":null,"content":"I am a PhD student at Stanford University advised by Prof. Leonidas Guibas. My research focuses on leveraging learned and physics-based motion models to improve 3D perception and synthesis of dynamic objects and humans.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b0e39f164d5da208e93d3678590967e7","permalink":"/author/davis-rempe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/davis-rempe/","section":"authors","summary":"I am a PhD student at Stanford University advised by Prof. Leonidas Guibas. My research focuses on leveraging learned and physics-based motion models to improve 3D perception and synthesis of dynamic objects and humans.","tags":null,"title":"Davis Rempe","type":"authors"},{"authors":["rafidmahmood"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b4ba619dd5ed502941db478c875271c","permalink":"/author/rafid-mahmood/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rafid-mahmood/","section":"authors","summary":"","tags":null,"title":"Rafid Mahmood","type":"authors"},{"authors":["janickmartinezesturo"],"categories":null,"content":"I am a Senior Research Engineer at NVIDIA\u0026rsquo;s Toronto AI Lab based in Munich. I\u0026rsquo;m excited about the geometric, computational, and probabilistic aspects of challenging vision, robotics, and graphics, and visualization problems. My research background includes interactive tools for shape modeling/editing, localization from heterogenous data, and geometry-based flow visualization. More recently, I productized self-calibration methods for diverse automotive sensors. My current focus is related to applying neural methods to enhance automotive re-simulations at scale.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1df99e4254100020ac42b44f95c9380a","permalink":"/author/janick-martinez-esturo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/janick-martinez-esturo/","section":"authors","summary":"I am a Senior Research Engineer at NVIDIA\u0026rsquo;s Toronto AI Lab based in Munich. I\u0026rsquo;m excited about the geometric, computational, and probabilistic aspects of challenging vision, robotics, and graphics, and visualization problems.","tags":null,"title":"Janick Martinez Esturo","type":"authors"},{"authors":["jasonpeng"],"categories":null,"content":"I\u0026rsquo;m currently a Ph.D. student at UC Berkeley advised by Professor Sergey Levine and Professor Pieter Abbeel. I received an M.Sc from the University of British Columbia, advised by Professor Michiel van de Panne. My work lies in the intersection between computer graphics and machine learning, with a focus on reinforcement learning for motion control of simulated characters.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c41fc4ac8c4ce6e34bf21a5d9edbbd51","permalink":"/author/jason-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jason-peng/","section":"authors","summary":"I\u0026rsquo;m currently a Ph.D. student at UC Berkeley advised by Professor Sergey Levine and Professor Pieter Abbeel. I received an M.Sc from the University of British Columbia, advised by Professor Michiel van de Panne.","tags":null,"title":"Jason Peng","type":"authors"},{"authors":["orperel"],"categories":null,"content":"I\u0026rsquo;m a Research Engineer at NVIDIA Toronto AI Lab. My research interests are at the intersection of 3D vision, computer graphics and ML. My mission is to build creative, neural-based tools for 3D content creation. My recent focus is around representation, rendering and manipulation of neural implicit functions/fields.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"82d4cdb8f0c8ec87ce68491903b81f7b","permalink":"/author/or-perel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/or-perel/","section":"authors","summary":"I\u0026rsquo;m a Research Engineer at NVIDIA Toronto AI Lab. My research interests are at the intersection of 3D vision, computer graphics and ML. My mission is to build creative, neural-based tools for 3D content creation.","tags":null,"title":"Or Perel","type":"authors"},{"authors":["despoinapaschalidou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"71062a171cd9dcbb0030b96d536ffd19","permalink":"/author/despoina-paschalidou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/despoina-paschalidou/","section":"authors","summary":"","tags":null,"title":"Despoina Paschalidou","type":"authors"},{"authors":["jonahphilion"],"categories":null,"content":"Website\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d881c29c284c9572f14b5f49c6c6d13e","permalink":"/author/jonah-philion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonah-philion/","section":"authors","summary":"Website","tags":null,"title":"Jonah Philion","type":"authors"},{"authors":["nicholassharp"],"categories":null,"content":"I’m a researcher in geometry processing, computer graphics/vision, and 3D machine learning. My work seeks new algorithms and new representations to make computing with geometric data easy, efficient, and reliable. Currently, I’m a Senior Research Scientist at NVIDIA, based out of Seattle, WA.\nPreviously I received my PhD in Computer Science from Carnegie Mellon University advised by Keenan Crane, and was a postdoc in the University of Toronto DGP with Alec Jacobson. Even earlier, I was an undergraduate at Virginia Tech, where I worked with T.M. Murali on computational systems biology, and was active in competitive programming. Outside of work, I’m a big fan of long-distance running, hockey, and cooking.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"123c8fa825497802de68e74ee12e3941","permalink":"/author/nicholas-sharp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nicholas-sharp/","section":"authors","summary":"I’m a researcher in geometry processing, computer graphics/vision, and 3D machine learning. My work seeks new algorithms and new representations to make computing with geometric data easy, efficient, and reliable. Currently, I’m a Senior Research Scientist at NVIDIA, based out of Seattle, WA.","tags":null,"title":"Nicholas Sharp","type":"authors"},{"authors":["gopalsharma"],"categories":null,"content":"I am a Ph.D. student at CICS UMass-Amherst. I am advised by Prof. Evangelos Kalogerakis, and Prof. Subhransu Maji. I work on learning interpretable and editable representation of shapes using neural networks. I also work on utilizing geometry for self-supervised representation learning for 3D shapes.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb609d3ad687961bd3b4d08eea3b24e0","permalink":"/author/gopal-sharma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gopal-sharma/","section":"authors","summary":"I am a Ph.D. student at CICS UMass-Amherst. I am advised by Prof. Evangelos Kalogerakis, and Prof. Subhransu Maji. I work on learning interpretable and editable representation of shapes using neural networks.","tags":null,"title":"Gopal Sharma","type":"authors"},{"authors":["katjaschwarz"],"categories":null,"content":"I am a PhD student in the Autonomous Vision Group (AVG) at Tübingen University and the Max Planck Institute for Intelligent Systems, advised by Andreas Geiger.\nMy research lies at the intersection of computer vision and graphics and focuses on 3D vision. In particular, I am interested in enabeling machines to infer 3D representations from sparse observations, such as 2D images. Further, I am passionate about generative modeling in 2D and 3D.\nI studied Physics at Heidelberg University where I received my bachelor degree in 2016 and master degree in 2018. In July 2019, I started my PhD in computer vision / machine learning in the Autonomous Vision Group (AVG) at Tübingen University and the Max Planck Institute for Intelligent Systems in Tübingen, Germany, under the supervision of Andreas Geiger.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2d9606dbbf6754683fe323b037024699","permalink":"/author/katja-schwarz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/katja-schwarz/","section":"authors","summary":"I am a PhD student in the Autonomous Vision Group (AVG) at Tübingen University and the Max Planck Institute for Intelligent Systems, advised by Andreas Geiger.\nMy research lies at the intersection of computer vision and graphics and focuses on 3D vision.","tags":null,"title":"Katja Schwarz","type":"authors"},{"authors":["frankshen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3dd516552086188aec3e991458f3bf5","permalink":"/author/frank-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/frank-shen/","section":"authors","summary":"","tags":null,"title":"Frank Shen","type":"authors"},{"authors":["weiweisun"],"categories":null,"content":"I am a Ph.D. student in Computer Science, University of British Columbia (UBC), co-advised by Dr. Kwang Moo Yi and Dr. Andrea Tagliasacchi. My research interests include 3D computer vision and deep learning. My primary focus recently is on 3D learning in the wild settings \u0026ndash; e.g., multi-view setting, real-world point clouds.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d59fab39584a19586271288a3e7eb673","permalink":"/author/weiwei-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weiwei-sun/","section":"authors","summary":"I am a Ph.D. student in Computer Science, University of British Columbia (UBC), co-advised by Dr. Kwang Moo Yi and Dr. Andrea Tagliasacchi. My research interests include 3D computer vision and deep learning.","tags":null,"title":"Weiwei Sun","type":"authors"},{"authors":["choyingwu"],"categories":null,"content":"Cho-Ying is a 5-th year PhD student at University of Southern California advised by Prof. Ulrich Neumann. His research lies in broad topics of 3D vision, including depth estimation from imagery for various scenes, indoor-focused AR/VR assisted depth estimation, and 3D face modeling from images or voices.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9db5e2d715ae121bb7fa0978a219ded9","permalink":"/author/cho-ying-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cho-ying-wu/","section":"authors","summary":"Cho-Ying is a 5-th year PhD student at University of Southern California advised by Prof. Ulrich Neumann. His research lies in broad topics of 3D vision, including depth estimation from imagery for various scenes, indoor-focused AR/VR assisted depth estimation, and 3D face modeling from images or voices.","tags":null,"title":"Cho-Ying Wu","type":"authors"},{"authors":["mashashugrina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b710e53d7696c37bd5f9e1c5b7f4147","permalink":"/author/masha-shugrina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/masha-shugrina/","section":"authors","summary":"","tags":null,"title":"Masha Shugrina","type":"authors"},{"authors":["towakitakikawa"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ce5d6218d34a263e6c634c625045dfa4","permalink":"/author/towaki-takikawa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/towaki-takikawa/","section":"authors","summary":"","tags":null,"title":"Towaki Takikawa","type":"authors"},{"authors":["tommyxiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ba5ce1113200696d0ce4a55f1555f978","permalink":"/author/tommy-xiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tommy-xiang/","section":"authors","summary":"","tags":null,"title":"Tommy Xiang","type":"authors"},{"authors":["tingwuwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"00033e991e190cf6e4a354975f38e86d","permalink":"/author/tingwu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tingwu-wang/","section":"authors","summary":"","tags":null,"title":"Tingwu Wang","type":"authors"},{"authors":["zianwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ff622ef7205d9df98598e14b66b9609","permalink":"/author/zian-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zian-wang/","section":"authors","summary":"","tags":null,"title":"Zian Wang","type":"authors"},{"authors":["franciswilliams"],"categories":null,"content":"I am a research scientist at NVIDIA in NYC working at the intersection of computer vision, machine learning, and computer graphics. My research is a mix of theory and application, aiming to solve practical problems in elegant ways. In particular, I’m very interested in 3D shape representations which can enable deep learning on “real-world” geometric datasets which are often noisy, unlabeled, and consisting of very large inputs.\nI completed my PhD from NYU in 2021 where I worked in the Math and Data Group and the Geometric Computing Lab. My advisors were Joan Bruna and Denis Zorin.\nIn addition to research, I also develop, maintain, and contribute to several open source projects. These include NumpyEigen, Point Cloud Utils, and FML.\nI’m currently looking for motivated interns to work with. Please reach out to me if you would like to chat about potential collaborations at NVIDIA!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"022d548e63965a209e867ffdb0125b1d","permalink":"/author/francis-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/francis-williams/","section":"authors","summary":"I am a research scientist at NVIDIA in NYC working at the intersection of computer vision, machine learning, and computer graphics. My research is a mix of theory and application, aiming to solve practical problems in elegant ways.","tags":null,"title":"Francis Williams","type":"authors"},{"authors":["kevinxie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b273d4708ae30cdc04370ea1a5d7ace7","permalink":"/author/kevin-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kevin-xie/","section":"authors","summary":"","tags":null,"title":"Kevin Xie","type":"authors"},{"authors":["xingguangyan"],"categories":null,"content":"Xingguang Yan is a Ph.D. student at Simon Fraser University supervised by Andrea Tagliasacchi. His research focuses on how to efficiently represent, process, and generate 3D data, spanning the fields of computer graphics, computer vision, and machine learning. More specifically, he is currently working on building 3D generative models for synthesizing high-quality 3D objects and scenes.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d41432e8bb6f5ee16dedffd4b5bd2fa","permalink":"/author/xingguang-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xingguang-yan/","section":"authors","summary":"Xingguang Yan is a Ph.D. student at Simon Fraser University supervised by Andrea Tagliasacchi. His research focuses on how to efficiently represent, process, and generate 3D data, spanning the fields of computer graphics, computer vision, and machine learning.","tags":null,"title":"Xingguang Yan","type":"authors"},{"authors":["junlinyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d45085dc6bc342fd9b9bcd6ee2ed7c62","permalink":"/author/junlin-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junlin-yang/","section":"authors","summary":"","tags":null,"title":"Junlin Yang","type":"authors"},{"authors":["kangxueyin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"32703f897bd8424fc595e7af09cf9d14","permalink":"/author/kangxue-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kangxue-yin/","section":"authors","summary":"","tags":null,"title":"Kangxue Yin","type":"authors"},{"authors":["xiaohuizeng"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD student at the University of Toronto. I am interested in generative model on 2D image and 3D objects. At NVIDIA, I\u0026rsquo;m working with 3D shapes generation.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"23229d34f714feca698deed587348fbb","permalink":"/author/xiaohui-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaohui-zeng/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD student at the University of Toronto. I am interested in generative model on 2D image and 3D objects. At NVIDIA, I\u0026rsquo;m working with 3D shapes generation.","tags":null,"title":"Xiaohui Zeng","type":"authors"},{"authors":["dongsuzhang"],"categories":null,"content":"I am a research intern at NVIDIA. I obtained my MS and BS at Seoul National University. My long term research goal is to create a machine that can persistently understand the wide-range of the spatial world. To do so, I work on 3D generative models, especially large-scale 3D generative models, which implicitly learns the essential features of the space by generating one.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"53cdcdf2c9be75ecc07388816e7bd06a","permalink":"/author/dongsu-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dongsu-zhang/","section":"authors","summary":"I am a research intern at NVIDIA. I obtained my MS and BS at Seoul National University. My long term research goal is to create a machine that can persistently understand the wide-range of the spatial world.","tags":null,"title":"Dongsu Zhang","type":"authors"},{"authors":["alexzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"df402183d6954aec8a0979fda4aac6ab","permalink":"/author/alex-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-zhang/","section":"authors","summary":"","tags":null,"title":"Alex Zhang","type":"authors"},{"authors":["haotianzhang"],"categories":null,"content":"I am a PhD student in Computer Science Department at Stanford University, advised by Prof. Kayvon Fatahalian. My research interest includes video analysis, editing and synthesis.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9a6b23941a1a7db1d9c78e10a4b201e0","permalink":"/author/haotian-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haotian-zhang/","section":"authors","summary":"I am a PhD student in Computer Science Department at Stanford University, advised by Prof. Kayvon Fatahalian. My research interest includes video analysis, editing and synthesis.\nWebsite","tags":null,"title":"Haotian Zhang","type":"authors"},{"authors":["siyanzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0721bfdb7795b0de16a0ba459c4719ae","permalink":"/author/siyan-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siyan-zhao/","section":"authors","summary":"","tags":null,"title":"Siyan Zhao","type":"authors"},{"authors":["alexbie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fcb08531a5bf13e454f0ff379c929ea9","permalink":"/author/alex-bie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-bie/","section":"authors","summary":"","tags":null,"title":"Alex Bie","type":"authors"},{"authors":["guojunzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"97d79e79ff77e9981453b8f24aba6fcf","permalink":"/author/guojun-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guojun-zhang/","section":"authors","summary":"","tags":null,"title":"Guojun Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics. Our group members are also part of or closely collaborate with academic labs such as the University of Toronto and Vector Institute.\nWe invite applications for the following positions:\n full time research scientist full time research engineer research scientist intern research engineer intern  Graduate and senior undergraduate students interested in doing an internship in the NVIDIA Toronto AI Lab can directly fill this form. See this link for open positions, or contact our members for more details.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/toronto-ai-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/toronto-ai-lab/","section":"authors","summary":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics.","tags":null,"title":"Toronto AI Lab","type":"authors"},{"authors":["Tim Dockhorn","Arash Vahdat","Karsten Kreis"],"categories":null,"content":"","date":1664724239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664724239,"objectID":"1bb5046dd74724fd063f879d1799cb6e","permalink":"/publication/2022_neurips_genie/","publishdate":"2022-10-02T11:23:59-04:00","relpermalink":"/publication/2022_neurips_genie/","section":"publication","summary":"Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE) defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling.","tags":[],"title":"GENIE: Higher-Order Denoising Diffusion Solvers","type":"publication"},{"authors":["Jun Gao","Tianchang Shen","Zian Wang","Wenzheng Chen","Kangxue Yin","Daiqing Li","Or Litany","Zan Gojcic","Sanja Fidler"],"categories":null,"content":"","date":1664724239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664724239,"objectID":"91f5a957a396e60f95f8e18ecd622d78","permalink":"/publication/2022_neurips_get3d/","publishdate":"2022-10-02T11:23:59-04:00","relpermalink":"/publication/2022_neurips_get3d/","section":"publication","summary":"As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.","tags":[],"title":"GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images","type":"publication"},{"authors":["Xiaohui Zeng","Arash Vahdat","Francis Williams","Zan Gojcic","Or Litany","Sanja Fidler","Karsten Kreis"],"categories":null,"content":"","date":1664724239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664724239,"objectID":"1a8ac3bb7a444bdc05537bd666ca1669","permalink":"/publication/2022_neurips_lion/","publishdate":"2022-10-02T11:23:59-04:00","relpermalink":"/publication/2022_neurips_lion/","section":"publication","summary":"Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text- and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction.","tags":[],"title":"LION: Latent Point Diffusion Models for 3D Shape Generation","type":"publication"},{"authors":["Rafid Mahmood","James Lucas","Jose M. Alvarez","Sanja Fidler","Marc T. Law"],"categories":null,"content":"","date":1664724239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664724239,"objectID":"a196f8df1c54f5981224494bc9c47843","permalink":"/publication/2022_neurips_data_collection/","publishdate":"2022-10-02T11:23:59-04:00","relpermalink":"/publication/2022_neurips_data_collection/","section":"publication","summary":"Modern deep learning systems require huge data sets to achieve impressive performance, but there is little guidance on how much or what kind of data to collect. Over-collecting data incurs unnecessary present costs, while under-collecting may incur future costs and delay workflows. We propose a new paradigm for modeling the data collection workflow as a formal optimal data collection problem that allows designers to specify performance targets, collection costs, a time horizon, and penalties for failing to meet the targets. Additionally, this formulation generalizes to tasks requiring multiple data sources, such as labeled and unlabeled data used in semi-supervised learning. To solve our problem, we develop Learn-Optimize-Collect (LOC), which minimizes expected future collection costs. Finally, we numerically compare our framework to the conventional baseline of estimating data requirements by extrapolating from neural scaling laws. We significantly reduce the risks of failing to meet desired performance targets on several classification, segmentation, and detection tasks, while maintaining low total collection costs.","tags":[],"title":"Optimizing data collection for machine learning","type":"publication"},{"authors":["Towaki Takikawa","Or Perel","Clement Fuji Tsang","Charles Loop","Joey Litalien","Jonathan Tremblay","Sanja Fidler","Maria Shugrina"],"categories":["3D","NeRF"],"content":"News blogpost\n","date":1660455187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660455187,"objectID":"8e047083dfda4151168e0dd3e515951b","permalink":"/publication/kaolinwisp/","publishdate":"2022-08-14T01:33:07-04:00","relpermalink":"/publication/kaolinwisp/","section":"publication","summary":"NVIDIA Kaolin Wisp is a PyTorch library powered by [NVIDIA Kaolin Core](https://nv-tlabs.github.io/publication/kaolin/) to work with neural fields (including NeRFs, NGLOD, instant-ngp and VQAD).\nNVIDIA Kaolin Wisp aims to provide a set of common utility functions for performing research on neural fields. This includes datasets, image I/O, mesh processing, and ray utility functions. Wisp also comes with building blocks like differentiable renderers and differentiable data structures (like octrees, hash grids, triplanar features) which are useful to build complex neural fields. It also includes debugging visualization tools, interactive rendering and training, logging, and trainer classes. ","tags":["3D","NeRF"],"title":"Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research","type":"publication"},{"authors":["Dávid Rozenberszki","Or Litany","Angela Dai"],"categories":null,"content":"","date":1659453839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659453839,"objectID":"cb19d2f932421155284d06e51eda0ee3","permalink":"/publication/2022_eccv_3d_segmentation/","publishdate":"2022-08-02T11:23:59-04:00","relpermalink":"/publication/2022_eccv_3d_segmentation/","section":"publication","summary":"Recent advances in 3D semantic segmentation with deep neural networks have shown remarkable success, with rapid performance increase on available datasets. However, current 3D semantic segmentation benchmarks contain only a small number of categories -- less than 30 for ScanNet and SemanticKITTI, for instance, which are not enough to reflect the diversity of real environments (e.g., semantic image understanding covers hundreds to thousands of classes). Thus, we propose to study a larger vocabulary for 3D semantic segmentation with a new extended benchmark on ScanNet data with 200 class categories, an order of magnitude more than previously studied. This large number of class categories also induces a large natural class imbalance, both of which are challenging for existing 3D semantic segmentation methods. To learn more robust 3D features in this context, we propose a language-driven pre-training method to encourage learned 3D features that might have limited training examples to lie close to their pre-trained text embeddings. Extensive experiments show that our approach consistently outperforms state-of-the-art 3D pre-training for 3D semantic segmentation on our proposed benchmark (+9% relative mIoU), including limited-data scenarios with +25% relative mIoU using only 5% annotations.","tags":[],"title":"Language-Grounded Indoor 3D Semantic Segmentation in the Wild","type":"publication"},{"authors":["Gopal Sharma","Kangxue Yin","Subhransu Maji","Evangelos Kalogerakis","Or Litany","Sanja Fidler"],"categories":null,"content":"","date":1659453839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659453839,"objectID":"63a88658013f83c52a8063b5e55543cc","permalink":"/publication/2022_eccv_mvdecor/","publishdate":"2022-08-02T11:23:59-04:00","relpermalink":"/publication/2022_eccv_mvdecor/","section":"publication","summary":"We propose to utilize self-supervised techniques in the 2D domain for fine-grained 3D shape segmentation tasks. This is inspired by the observation that view-based surface representations are more effective at modeling high-resolution surface details and texture than their 3D counterparts based on point clouds or voxel occupancy. Specifically, given a 3D shape, we render it from multiple views, and set up a dense correspondence learning task within the contrastive learning framework. As a result, the learned 2D representations are view-invariant and geometrically consistent, leading to better generalization when trained on a limited number of labeled shapes compared to alternatives that utilize self-supervision in 2D or 3D alone. Experiments on textured (RenderPeople) and untextured (PartNet) 3D datasets show that our method outperforms state-of-the-art alternatives in fine-grained part segmentation. The improvements over baselines are greater when only a sparse set of views is available for training or when shapes are textured, indicating that MvDeCor benefits from both 2D processing and 3D geometric reasoning.","tags":[],"title":"MvDeCor: Multi-view Dense Correspondence Learning for Fine-grained 3D Segmentation","type":"publication"},{"authors":["Derek Liu","Francis Williams","Alec Jacobson","Sanja Fidler","Or Litany"],"categories":null,"content":"","date":1656775499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656775499,"objectID":"7a617b1fb3a56955b65b6a8b9a379d66","permalink":"/publication/2022_siggraph_smooth_neural_functions/","publishdate":"2022-07-02T11:24:59-04:00","relpermalink":"/publication/2022_siggraph_smooth_neural_functions/","section":"publication","summary":"Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.","tags":[],"title":"Learning Smooth Neural Functions via Lipschitz Regularization","type":"publication"},{"authors":["Xue Bin Peng","Yunrong Guo","Lina Halper","Sergey Levine","Sanja Fidler"],"categories":null,"content":"","date":1656775439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656775439,"objectID":"a67ba267070f9a946dfef85bfcd8d13f","permalink":"/publication/2022_siggraph_ase/","publishdate":"2022-07-02T11:23:59-04:00","relpermalink":"/publication/2022_siggraph_ase/","section":"publication","summary":"The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and experience. These skills not only enable humans to perform complex tasks, but also provide powerful priors for guiding their behaviors when learning new tasks. This is in stark contrast to what is common practice in physics-based character animation, where control policies are most typically trained from scratch for each task. In this work, we present a large-scale data-driven framework for learning versatile and reusable skill embeddings for physically simulated characters. Our approach combines techniques from adversarial imitation learning and unsupervised reinforcement learning to develop skill embeddings that produce life-like behaviors, while also providing an easy to control representation for use on new downstream tasks. Our models can be trained using large datasets of unstructured motion clips, without requiring any task-specific annotation or segmentation of the motion data. By leveraging a massively parallel GPU-based simulator, we are able to train skill embeddings using over a decade of simulated experiences, enabling our model to learn a rich and versatile repertoire of skills. We show that a single pre-trained model can be effectively applied to perform a diverse set of new tasks. Our system also allows users to specify tasks through simple reward functions, and the skill embedding then enables the character to automatically synthesize complex and naturalistic strategies in order to achieve the task objectives.","tags":[],"title":"ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters","type":"publication"},{"authors":["Towaki Takikawa","Alex Evans","Jonathan Tremblay","Thomas Müller","Morgan McGuire","Alec Jacobson","Sanja Fidler"],"categories":null,"content":"","date":1656775439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656775439,"objectID":"b33ce3be8507b7ce3c58e370b723f179","permalink":"/publication/2022_siggraph_variable_bitrate/","publishdate":"2022-07-02T11:23:59-04:00","relpermalink":"/publication/2022_siggraph_variable_bitrate/","section":"publication","summary":"Neural approximations of scalar and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100x and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure.","tags":[],"title":"Variable Bitrate Neural Fields","type":"publication"},{"authors":["Seung Wook Kim","Karsten Kreis","Daiqing Li","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1653233039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653233039,"objectID":"54850bf11e139f34752c5e9e276fbd91","permalink":"/publication/2022_cvpr_polymorphic_gan/","publishdate":"2022-05-22T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_polymorphic_gan/","section":"publication","summary":"Modern image generative models show remarkable sample quality when trained on a single domain or class of objects. In this work, we introduce a generative adversarial network that can simultaneously generate aligned image samples from multiple related domains. We leverage the fact that a variety of object classes share common attributes, with certain geometric differences. We propose Polymorphic-GAN which learns shared features across all domains and a per-domain morph layer to morph shared features according to each domain. In contrast to previous works, our framework allows simultaneous modelling of images with highly varying geometries, such as images of human faces, painted and artistic faces, as well as multiple different animal faces. We demonstrate that our model produces aligned samples for all domains and show how it can be used for applications such as segmentation transfer and cross-domain image editing, as well as training in low-data regimes. Additionally, we apply our Polymorphic-GAN on image-to-image translation tasks and show that we can greatly surpass previous approaches in cases where the geometric differences between domains are large.","tags":[],"title":"Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps","type":"publication"},{"authors":["Jacob Munkberg","Jon Hasselgren","Tianchang Shen","Jun Gao","Wenzheng Chen","Alex Evans","Thomas Müller","Sanja Fidler"],"categories":null,"content":"","date":1653146639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653146639,"objectID":"ad78968020aa49e2f189b775773e8c46","permalink":"/publication/2022_cvpr_triangular/","publishdate":"2022-05-21T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_triangular/","section":"publication","summary":"We present an efficient method for joint optimization of topology, materials and lighting from multi-view image observations. Unlike recent multi-view reconstruction approaches, which typically produce entangled 3D representations encoded in neural networks, we output triangle meshes with spatially-varying materials and environment lighting that can be deployed in any traditional graphics engine unmodified. We leverage recent work in differentiable rendering, coordinate-based networks to compactly represent volumetric texturing, alongside differentiable marching tetrahedrons to enable gradient-based optimization directly on the surface mesh. Finally, we introduce a differentiable formulation of the split sum approximation of environment lighting to efficiently recover all-frequency lighting. Experiments show our extracted models used in advanced scene editing, material decomposition, and high quality view interpolation, all running at interactive rates in triangle-based renderers (rasterizers and path tracers).","tags":[],"title":"Extracting Triangular 3D Models, Materials, and Lighting From Images","type":"publication"},{"authors":["Zhiqin Chen","Kangxue Yin","Sanja Fidler"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"2f1f619085c54fac5872997492b8e8da","permalink":"/publication/2022_cvpr_auv/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_auv/","section":"publication","summary":"In this paper, we address the problem of texture representation for 3D shapes for the challenging and underexplored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the traditional way of representing textures with images and linking them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding semantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applications including texture transfer, texture synthesis, and textured single view 3D reconstruction. We conduct experiments on multiple datasets to demonstrate the effectiveness of our method.","tags":[],"title":"AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis","type":"publication"},{"authors":["Daiqing Li","Huan Ling","Seung Wook Kim","Karsten Kreis","Adela Barriuso","Sanja Fidler","Antonio Torralba"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"4e26df59000a4f50c9ecf94725077591","permalink":"/publication/2022_cvpr_big_datasetgan/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_big_datasetgan/","section":"publication","summary":"Annotating images with pixel-wise labels is a time-consuming and costly process. Recently, DatasetGAN showcased a promising alternative - to synthesize a large labeled dataset via a generative adversarial network (GAN) by exploiting a small set of manually labeled, GAN-generated images. Here, we scale DatasetGAN to ImageNet scale of class diversity. We take image samples from the class-conditional generative model BigGAN trained on ImageNet, and manually annotate 5 images per class, for all 1k classes. By training an effective feature segmentation architecture on top of BigGAN, we turn BigGAN into a labeled dataset generator. We further show that VQGAN can similarly serve as a dataset generator, leveraging the already annotated data. We create a new ImageNet benchmark by labeling an additional set of 8k real images and evaluate segmentation performance in a variety of settings. Through an extensive ablation study we show big gains in leveraging a large generated dataset to train different supervised and self-supervised backbone models on pixel-wise tasks. Furthermore, we demonstrate that using our synthesized datasets for pre-training leads to improvements over standard ImageNet pre-training on several downstream datasets, such as PASCAL-VOC, MS-COCO, Cityscapes and chest X-ray, as well as tasks (detection, segmentation). Our benchmark will be made public and maintain a leaderboard for this challenging task","tags":[],"title":"BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations","type":"publication"},{"authors":["Matan Atzmon","Koki Nagano","Sanja Fidler","Sameh Khamis","Yaron Lipman"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"91d77ac2564af6f4b439dc044a0b22f8","permalink":"/publication/2022_cvpr_equivarient/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_equivarient/","section":"publication","summary":"The task of shape space learning involves mapping a train set of shapes to and from a latent representation space with good generalization properties. Often, real-world collections of shapes have symmetries, which can be defined as transformations that do not change the essence of the shape. A natural way to incorporate symmetries in shape space learning is to ask that the mapping to the shape space (encoder) and mapping from the shape space (decoder) are equivariant to the relevant symmetries. In this paper, we present a framework for incorporating equivariance in encoders and decoders by introducing two contributions: (i) adapting the recent Frame Averaging (FA) framework for building generic, efficient, and maximally expressive Equivariant autoencoders; and (ii) constructing autoencoders equivariant to piecewise Euclidean motions applied to different parts of the shape. To the best of our knowledge, this is the first fully piecewise Euclidean equivariant autoencoder construction. Training our framework is simple: it uses standard reconstruction losses and does not require the introduction of new losses. Our architectures are built of standard (backbone) architectures with the appropriate frame averaging to make them equivariant. Testing our framework on both rigid shapes dataset using implicit neural representations, and articulated shape datasets using mesh-based neural networks show state-of-the-art generalization to unseen test shapes, improving relevant baselines by a large margin. In particular, our method demonstrates significant improvement in generalizing to unseen articulated poses.","tags":[],"title":"Frame Averaging for Equivariant Shape Space Learning","type":"publication"},{"authors":["Davis Rempe","Jonah Philion","Leonidas Guibas","Sanja Fidler","Or Litany"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"74c452aa9d10000c7230e322e3b8aa89","permalink":"/publication/2022_cvpr_accident_prone/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_accident_prone/","section":"publication","summary":"Evaluating and improving planning for autonomous vehicles requires scalable generation of long-tail traffic scenarios. To be useful, these scenarios must be realistic and challenging, but not impossible to drive through safely. In this work, we introduce STRIVE, a method to automatically generate challenging scenarios that cause a given planner to produce undesirable behavior, like collisions. To maintain scenario plausibility, the key idea is to leverage a learned model of traffic motion in the form of a graph-based conditional VAE. Scenario generation is formulated as an optimization in the latent space of this traffic model, perturbing an initial real-world scene to produce trajectories that collide with a given planner. A subsequent optimization is used to find a solution to the scenario, ensuring it is useful to improve the given planner. Further analysis clusters generated scenarios based on collision type. We attack two planners and show that STRIVE successfully generates realistic, challenging scenarios in both cases. We additionally close the loop and use these scenarios to optimize hyperparameters of a rule-based planner.","tags":[],"title":"Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior","type":"publication"},{"authors":["Rafid Mahmood","James Lucas","David Acuna","Daiqing Li","Jonah Philion","Jose M. Alvarez","Zhiding Yu","Sanja Fidler","Marc T. Law"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"e6a3bfc286fbd994480a8ea8168ea9e8","permalink":"/publication/2022_cvpr_how_much_data/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_how_much_data/","section":"publication","summary":"Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.","tags":[],"title":"How Much More Data Do I Need? Estimating Requirements for Downstream Tasks","type":"publication"},{"authors":["Francis Williams","Zan Gojcic","Sameh Khamis","Denis Zorin","Joan Bruna","Sanja Fidler","Or Litany"],"categories":null,"content":"","date":1653060239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653060239,"objectID":"256fae0d64d5f9eb097440a1c384ef68","permalink":"/publication/2022_cvpr_nkf/","publishdate":"2022-05-20T11:23:59-04:00","relpermalink":"/publication/2022_cvpr_nkf/","section":"publication","summary":"We present Neural Kernel Fields: a novel method for reconstructing implicit 3D shapes based on a learned kernel ridge regression. Our technique achieves state-of-the-art results when reconstructing 3D objects and large scenes from sparse oriented points, and can reconstruct shape categories outside the training set with almost no drop in accuracy. The core insight of our approach is that kernel methods are extremely effective for reconstructing shapes when the chosen kernel has an appropriate inductive bias. We thus factor the problem of shape reconstruction into two parts: (1) a backbone neural network which learns kernel parameters from data, and (2) a kernel ridge regression that fits the input points on-the-fly by solving a simple positive definite linear system using the learned kernel. As a result of this factorization, our reconstruction gains the benefits of data-driven methods under sparse point density while maintaining interpolatory behavior, which converges to the ground truth shape as input sampling density increases. Our experiments demonstrate a strong generalization capability to objects outside the train-set category and scanned scenes.","tags":[],"title":"Neural Fields as Learnable Kernels for 3D Reconstruction","type":"publication"},{"authors":["Tim Dockhorn","Arash Vahdat","Karsten Kreis"],"categories":null,"content":"","date":1643815499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643815499,"objectID":"26b12f0abb1983f043831f133c59b0e4","permalink":"/publication/2022_langevin_diffusion/","publishdate":"2022-02-02T11:24:59-04:00","relpermalink":"/publication/2022_langevin_diffusion/","section":"publication","summary":"Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered 'velocities' that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis.","tags":[],"title":"Score-Based Generative Modeling with Critically-Damped Langevin Diffusion","type":"publication"},{"authors":["Zhisheng Xiao","Karsten Kreis","Arash Vahdat"],"categories":null,"content":"","date":1643815498,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643815498,"objectID":"de45ca379fd492201c7963a1dd16f00e","permalink":"/publication/2022_trilemma/","publishdate":"2022-02-02T11:24:58-04:00","relpermalink":"/publication/2022_trilemma/","section":"publication","summary":"A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000× faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.","tags":[],"title":"Tackling the Generative Learning Trilemma with Denoising Diffusion GANs","type":"publication"},{"authors":["David Acuna","Marc T. Law","Guojun Zhang","Sanja Fidler"],"categories":null,"content":"","date":1643811899,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643811899,"objectID":"2e0343d13e473795b8ee97e150336a31","permalink":"/publication/2022_iclr_game/","publishdate":"2022-02-02T10:24:59-04:00","relpermalink":"/publication/2022_iclr_game/","section":"publication","summary":"The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective. Defining optimal solutions in domain-adversarial training as a local Nash equilibrium, we show that gradient descent in domain-adversarial training can violate the asymptotic convergence guarantees of the optimizer, oftentimes hindering the transfer performance. Our analysis leads us to replace gradient descent with high-order ODE solvers (i.e., Runge–Kutta), for which we derive asymptotic convergence guarantees. This family of optimizers is significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers. Our experiments show that in conjunction with state-of-the-art domain-adversarial methods, we achieve up to 3.5% improvement with less than of half training iterations. Our optimizers are easy to implement, free of additional parameters, and can be plugged into any domain-adversarial framework.","tags":[],"title":"Domain Adversarial Training: A Game Perspective","type":"publication"},{"authors":["Rafid Mahmood","Sanja Fidler","Marc T. Law"],"categories":null,"content":"","date":1643811839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643811839,"objectID":"0da76544e6331fec77dda16d0d7ebabf","permalink":"/publication/2022_iclr_active_learning/","publishdate":"2022-02-02T10:23:59-04:00","relpermalink":"/publication/2022_iclr_active_learning/","section":"publication","summary":"Active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics. This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled pool. We demonstrate that this problem can be tractably solved with a Generalized Benders Decomposition algorithm. Our strategy uses high-quality latent features that can be obtained by unsupervised learning on the unlabeled pool. Numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled. ","tags":[],"title":"Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach","type":"publication"},{"authors":["Luca Moschella","Simone Melzi","Luca Cosmo","Filippo Maggioli","Or Litany","Maks Ovsjanikov","Leonidas Guibas","Emanuele Rodolà"],"categories":null,"content":"","date":1638458639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638458639,"objectID":"b9c3d98ccbf57126bfa02ffc6351360a","permalink":"/publication/eurographics_2022_spectral_unions/","publishdate":"2021-12-02T11:23:59-04:00","relpermalink":"/publication/eurographics_2022_spectral_unions/","section":"publication","summary":"Spectral geometric methods have brought revolutionary changes to the field of geometry processing -- however, when the data to be processed exhibits severe partiality, such methods fail to generalize. As a result, there exists a big performance gap between methods dealing with complete shapes, and methods that address missing geometry. In this paper, we propose a possible way to fill this gap. We introduce the first method to compute compositions of non-rigidly deforming shapes, without requiring to solve first for a dense correspondence between the given partial shapes. We do so by operating in a purely spectral domain, where we define a union operation between short sequences of eigenvalues. Working with eigenvalues allows to deal with unknown correspondence, different sampling, and different discretization (point clouds and meshes alike), making this operation especially robust and general. Our approach is data-driven, and can generalize to isometric and non-isometric deformations of the surface, as long as these stay within the same semantic class (e.g., human bodies), as well as to partiality artifacts not seen at training time.","tags":[],"title":"Spectral Unions of Partial Deformable 3D Shapes","type":"publication"},{"authors":["Evgenii Zheltonozhskii","Chaim Baskin","Avi Mendelson","Alex M. Bronstein","Or Litany"],"categories":null,"content":"","date":1638372239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638372239,"objectID":"e5657e8a85f4c4d2dcb566d7f58a1e14","permalink":"/publication/wacv_2022_contrast_to_divide/","publishdate":"2021-12-01T11:23:59-04:00","relpermalink":"/publication/wacv_2022_contrast_to_divide/","section":"publication","summary":"The success of learning with noisy labels (LNL) methods relies heavily on the success of a warm-up stage where standard supervised training is performed using the full (noisy) training set. In this paper, we identify a warm-up obstacle: the inability of standard warm-up stages to train high quality feature extractors and avert memorization of noisy labels. We propose Contrast to Divide (C2D), a simple framework that solves this problem by pre-training the feature extractor in a self-supervised fashion. Using self-supervised pre-training boosts the performance of existing LNL approaches by drastically reducing the warm-up stage's susceptibility to noise level, shortening its duration, and improving extracted feature quality. C2D works out of the box with existing methods and demonstrates markedly improved performance, especially in the high noise regime, where we get a boost of more than 27% for CIFAR-100 with 90% noise over the previous state of the art. In real-life noise settings, C2D trained on mini-WebVision outperforms previous works both in WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an in-depth analysis of the framework, including investigating the performance of different pre-training approaches and estimating the effective upper bound of the LNL performance with semi-supervised learning.","tags":[],"title":"Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels","type":"publication"},{"authors":["Despoina Paschalidou","Amlan Kar","Maria Shugrina","Karsten Kreis","Andreas Geiger","Sanja Fidler"],"categories":null,"content":"","date":1635780239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635780239,"objectID":"43866d6323f07bbecc1c8e5a0f5054e1","permalink":"/publication/neurips_2021_atiss/","publishdate":"2021-11-01T11:23:59-04:00","relpermalink":"/publication/neurips_2021_atiss/","section":"publication","summary":"The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.","tags":[],"title":"ATISS: Autoregressive Transformers for Indoor Scene Synthesis","type":"publication"},{"authors":["Tianchang Shen","Jun Gao","Kangxue Yin","Ming-Yu Liu","Sanja Fidler"],"categories":null,"content":"","date":1635780238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635780238,"objectID":"02ae70e3fff7ecb60a2ef53a9d7fe491","permalink":"/publication/neurips_2021_deep_marching_tetrahedra/","publishdate":"2021-11-01T11:23:58-04:00","relpermalink":"/publication/neurips_2021_deep_marching_tetrahedra/","section":"publication","summary":"We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes.","tags":[],"title":"Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis","type":"publication"},{"authors":["Arash Vahdat","Karsten Kreis","Jan Kautz"],"categories":null,"content":"","date":1635776639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635776639,"objectID":"333cbbee33cb9be334c742d633b08966","permalink":"/publication/neurips_2021_lsgm/","publishdate":"2021-11-01T10:23:59-04:00","relpermalink":"/publication/neurips_2021_lsgm/","section":"publication","summary":"Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset.","tags":[],"title":"Score-based Generative Modeling in Latent Space","type":"publication"},{"authors":["David Acuna","Jonah Philion","Sanja Fidler"],"categories":null,"content":"","date":1635690239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690239,"objectID":"14be48224b099ab05129e8982e585599","permalink":"/publication/neurips_2021_simulation_strategies/","publishdate":"2021-10-31T10:23:59-04:00","relpermalink":"/publication/neurips_2021_simulation_strategies/","section":"publication","summary":"Autonomous driving relies on a huge volume of real-world data to be labeled to high precision. Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the domain gap between the synthetic and real data remains, raising the following important question: What are the best ways to utilize a self-driving simulator for perception tasks? In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective, propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone. Our approach introduces both a principled way to learn neural-invariant representations and a theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator. We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes). Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design, and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique.","tags":[],"title":"Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation","type":"publication"},{"authors":["Marc T. Law"],"categories":null,"content":"","date":1635690179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690179,"objectID":"3a7f979069086d8dee92cd0483bf0578","permalink":"/publication/neurips_2021_ultrahyperbolic/","publishdate":"2021-10-31T10:22:59-04:00","relpermalink":"/publication/neurips_2021_ultrahyperbolic/","section":"publication","summary":"Riemannian space forms, such as the Euclidean space, sphere and hyperbolic space, are popular and powerful representation spaces in machine learning. For instance, hyperbolic geometry is appropriate to represent graphs without cycles and has been used to extend Graph Neural Networks. Recently, some pseudo-Riemannian space forms that generalize both hyperbolic and spherical geometries have been exploited to learn a specific type of nonparametric embedding called ultrahyperbolic. The lack of geodesic between every pair of ultrahyperbolic points makes the task of learning parametric models (e.g., neural networks) difficult. This paper introduces a method to learn parametric models in ultrahyperbolic space. We experimentally show the relevance of our approach in the tasks of graph and node classification. ","tags":[],"title":"Ultrahyperbolic Neural Networks","type":"publication"},{"authors":["Wenzheng Chen","Joey Litalien","Jun Gao","Zian Wang","Clement Fuji Tsang","Sameh Khamis","Or Litany","Sanja Fidler"],"categories":null,"content":"","date":1635690178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690178,"objectID":"d57d99f1d471875897298760ff18273b","permalink":"/publication/neurips_2021_dibr/","publishdate":"2021-10-31T10:22:58-04:00","relpermalink":"/publication/neurips_2021_dibr/","section":"publication","summary":"We consider the challenging problem of predicting intrinsic object properties from a single image by exploiting differentiable renderers. Many previous learning-based approaches for inverse graphics adopt rasterization-based renderers and assume naive lighting and material models, which often fail to account for non-Lambertian, specular reflections commonly observed in the wild. In this work, we propose DIBR++, a hybrid differentiable renderer which supports these photorealistic effects by combining rasterization and ray-tracing, taking the advantage of their respective strengths -- speed and realism. Our renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. Compared to more advanced physics-based differentiable renderers leveraging path tracing, DIBR++ is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. We experimentally demonstrate that our approach achieves superior material and lighting disentanglement on synthetic and real data compared to existing rasterization-based approaches and showcase several artistic applications including material editing and relighting.","tags":[],"title":"DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer","type":"publication"},{"authors":["Tianshi Cao","Alex Bie","Arash Vahdat","Sanja Fidler","Karsten Kreis"],"categories":null,"content":"","date":1635690119,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690119,"objectID":"11f5c139d27bd9ea9f4423a34ce2619b","permalink":"/publication/neurips_2021_differentially_private_generative/","publishdate":"2021-10-31T10:21:59-04:00","relpermalink":"/publication/neurips_2021_differentially_private_generative/","section":"publication","summary":"Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.","tags":[],"title":"Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence","type":"publication"},{"authors":["Huan Ling","Karsten Kreis","Daiqing Li","Seung Wook Kim","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1635690118,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690118,"objectID":"f5c97e409a5fd474d72821bd806e3a27","permalink":"/publication/neurips_2021_editgan/","publishdate":"2021-10-31T10:21:58-04:00","relpermalink":"/publication/neurips_2021_editgan/","section":"publication","summary":"Generative adversarial networks (GANs) have recently found applications in image editing. However, most GAN based image editing methods often require large scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high quality, high precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car. EditGAN builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. Specifically, we embed an image into the GAN latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, we find editing vectors in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. We demonstrate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks.","tags":[],"title":"EditGAN: High-Precision Semantic Image Editing","type":"publication"},{"authors":["Alexey Nekrasov","Jonas Schult","Or Litany","Bastian Leibe","Francis Engelmann"],"categories":null,"content":"","date":1635603718,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635603718,"objectID":"ff613aa086f0723b0fd4966885b66cc9","permalink":"/publication/3dv_2021_mix3d/","publishdate":"2021-10-30T10:21:58-04:00","relpermalink":"/publication/3dv_2021_mix3d/","section":"publication","summary":"We present Mix3D, a data augmentation technique for segmenting large-scale 3D scenes. Since scene context helps reasoning about object semantics, current works focus on models with large capacity and receptive fields that can fully capture the global context of an input 3D scene. However, strong contextual priors can have detrimental implications like mistaking a pedestrian crossing the street for a car. In this work, we focus on the importance of balancing global scene context and local geometry, with the goal of generalizing beyond the contextual priors in the training set. In particular, we propose a mixing technique which creates new training samples by combining two augmented scenes. By doing so, object instances are implicitly placed into novel out-of-context environments and therefore making it harder for models to rely on scene context alone, and instead infer semantics from local structure as well. We perform detailed analysis to understand the importance of global context, local structures and the effect of mixing scenes. In experiments, we show that models trained with Mix3D profit from a significant performance boost on indoor (ScanNet, S3DIS) and outdoor datasets (SemanticKITTI). Mix3D can be trivially used with any existing method, e.g., trained with Mix3D, MinkowskiNet outperforms all prior state-of-the-art methods by a significant margin on the ScanNet test benchmark 78.1 mIoU.","tags":[],"title":"Mix3D: Out-of-Context Data Augmentation for 3D Scenes","type":"publication"},{"authors":["Zian Wang","Jonah Philion","Sanja Fidler","Jan Kautz"],"categories":null,"content":"","date":1630419719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630419719,"objectID":"25e5d05b1a9c7e8c168ba467ed777744","permalink":"/publication/iccv_2021_inverse_rendering/","publishdate":"2021-08-31T10:21:59-04:00","relpermalink":"/publication/iccv_2021_inverse_rendering/","section":"publication","summary":"In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.","tags":[],"title":"Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting","type":"publication"},{"authors":["Kangxue Yin","Jun Gao","Maria Shugrina","Sameh Khamis","Sanja Fidler"],"categories":null,"content":"","date":1630333319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630333319,"objectID":"61127a945d849ad83b2cf32becbaa148","permalink":"/publication/iccv_2021_3dstylenet/","publishdate":"2021-08-30T10:21:59-04:00","relpermalink":"/publication/iccv_2021_3dstylenet/","section":"publication","summary":"We propose a method to create plausible geometric and texture style variations of 3D objects in the quest to democratize 3D content creation. Given a pair of textured source and target objects, our method predicts a part-aware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target. In addition, the texture style of the target is transferred to the warped source object with the help of a multi-view differentiable renderer. Our model, 3DStyleNet, is composed of two sub-networks trained in two stages. First, the geometric style network is trained on a large set of untextured 3D shapes. Second, we jointly optimize our geometric style network and a pre-trained image style transfer network with losses defined over both the geometry and the rendering of the result. Given a small set of high-quality textured objects, our method can create many novel stylized shapes, resulting in effortless 3D content creation and style-ware data augmentation. We showcase our approach qualitatively on 3D content stylization, and provide user studies to validate the quality of our results. In addition, our method can serve as a valuable tool to create 3D data augmentations for computer vision tasks. Extensive quantitative analysis shows that 3DStyleNet outperforms alternative data augmentation techniques for the downstream task of single-image 3D reconstruction.","tags":[],"title":"3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations","type":"publication"},{"authors":["Congyue Deng","Or Litany","Yueqi Duan","Adrien Poulenard","Andrea Tagliasacchi","Leonidas Guibas"],"categories":null,"content":"","date":1630246919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630246919,"objectID":"9b64908c712ea0bbd2b4b2684ba269e9","permalink":"/publication/iccv_2021_vector_neurons/","publishdate":"2021-08-29T10:21:59-04:00","relpermalink":"/publication/iccv_2021_vector_neurons/","section":"publication","summary":"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.","tags":[],"title":"Vector Neurons: A General Framework for SO(3)-Equivariant Networks","type":"publication"},{"authors":["Kevin Xie","Tingwu Wang","Umar Iqbal","Yunrong Guo","Sanja Fidler","Florian Shkurti"],"categories":null,"content":"","date":1630246918,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630246918,"objectID":"52f3e09fda74a1b3327178e3aceaca94","permalink":"/publication/iccv_2021_physics/","publishdate":"2021-08-29T10:21:58-04:00","relpermalink":"/publication/iccv_2021_physics/","section":"publication","summary":"Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. Samples from the generative model can optionally be further refined with the same physics correction optimization. We demonstrate both qualitatively and quantitatively significantly improved motion estimation, synthesis quality and physical plausibility achieved by our method on the large scale Human3.6m dataset as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis.","tags":[],"title":"Physics-based Human Motion Estimation and Synthesis from Videos","type":"publication"},{"authors":["Aayush Prakash","Shoubhik Debnath","Jean-Francois Lafleche","Eric Cameracci","Gavriel State","Stan Birchfield","Marc T. Law"],"categories":null,"content":"","date":1629555719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629555719,"objectID":"1fc4de8c03f3a7942bcc8f8be8e45adc","permalink":"/publication/iccv_2021_sim2sg/","publishdate":"2021-08-21T10:21:59-04:00","relpermalink":"/publication/iccv_2021_sim2sg/","section":"publication","summary":"Synthetic data is emerging as a promising solution to the scalability issue of supervised deep learning, especially when real data are difficult to acquire or hard to annotate. Synthetic data generation, however, can itself be prohibitively expensive when domain experts have to manually and painstakingly oversee the process. Moreover, neural networks trained on synthetic data often do not perform well on real data because of the domain gap. To solve these challenges, we propose Sim2SG, a self-supervised automatic scene generation technique for matching the distribution of real data. Importantly, Sim2SG does not require supervision from the real-world dataset, thus making it applicable in situations for which such annotations are difficult to obtain. Sim2SG is designed to bridge both the content and appearance gaps, by matching the content of real data, and by matching the features in the source and target domains. We select scene graph (SG) generation as the downstream task, due to the limited availability of labeled datasets. Experiments demonstrate significant improvements over leading baselines in reducing the domain gap both qualitatively and quantitatively, on several synthetic datasets as well as the real-world KITTI dataset.","tags":[],"title":"Self-Supervised Real-to-Sim Scene Generation","type":"publication"},{"authors":["David Acuna","Guojun Zhang","Marc T. Law","Sanja Fidler"],"categories":null,"content":"","date":1626099779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626099779,"objectID":"337e18597ffa2992824e3cf74d5ca9cf","permalink":"/publication/icml_2021_fdal/","publishdate":"2021-07-12T10:22:59-04:00","relpermalink":"/publication/icml_2021_fdal/","section":"publication","summary":"Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a) as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines, and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.","tags":[],"title":"f-Domain-Adversarial Learning: Theory and Algorithms","type":"publication"},{"authors":["Nadine Chang","Zhiding Yu","Yu-Xiong Wang","Anima Anandkumar","Sanja Fidler","Jose M. Alvarez"],"categories":null,"content":"","date":1626096179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626096179,"objectID":"edff1664ff7f4731ddca083a074c44f5","permalink":"/publication/icml_2021_long_tail/","publishdate":"2021-07-12T10:22:59-03:00","relpermalink":"/publication/icml_2021_long_tail/","section":"publication","summary":"Training on datasets with long-tailed distributions has been challenging for major recognition tasks such as classification and detection. To deal with this challenge, image resampling is typically introduced as a simple but effective approach. However, we observe that long-tailed detection differs from classification since multiple classes may be present in one image. As a result, image resampling alone is not enough to yield a sufficiently balanced distribution at the object level. We address object-level resampling by introducing an object-centric memory replay strategy based on dynamic, episodic memory banks. Our proposed strategy has two benefits: 1) convenient object-level resampling without significant extra computation, and 2) implicit feature-level augmentation from model updates. We show that image-level and object-level resamplings are both important, and thus unify them with a joint resampling strategy (RIO). Our method outperforms state-of-the-art long-tailed detection and segmentation methods on LVIS v0.5 across various backbones.","tags":[],"title":"Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection","type":"publication"},{"authors":["Yuxuan Zhang","Huan Ling","Jun Gao","Kangxue Yin","Jean-Francois Lafleche","Adela Barriuso","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"945e62e056e2517eef12fae1051ad8f1","permalink":"/publication/datasetgan/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/datasetgan/","section":"publication","summary":"We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods using labor intensive annotations.","tags":[],"title":"DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort","type":"publication"},{"authors":["Seung Wook Kim","Jonah Philion","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"89a313283a90b62755e1d9e58392b441","permalink":"/publication/cvpr_2021_drivegan/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_drivegan/","section":"publication","summary":"Realistic simulators are critical for training and verifying robotics systems. While most of the contemporary simulators are hand-crafted, a scaleable way to build simulators is to use machine learning to learn how the environment behaves in response to an action, directly from data. In this work, we aim to learn to simulate a dynamic environment directly in pixel-space, by watching unannotated sequences of frames and their associated action pairs. We introduce a novel high-quality neural simulator referred to as DriveGAN that achieves controllability by disentangling different components without supervision. In addition to steering controls, it also includes controls for sampling features of a scene, such as the weather as well as the location of non-player objects. Since DriveGAN is a fully differentiable simulator, it further allows for re-simulation of a given video sequence, offering an agent to drive through a recorded scene again, possibly taking different actions. We train DriveGAN on multiple datasets, including 160 hours of real-world driving data. We showcase that our approach greatly surpasses the performance of previous data-driven simulators, and allows for new features not explored before.","tags":[],"title":"DriveGAN: Towards a Controllable High-Quality Neural Simulation","type":"publication"},{"authors":["Towaki Takikawa","Joey Litalien","Kangxue Yin","Karsten Kreis","Charles Loop","Derek Nowrouzezahrai","Alec Jacobson","Morgan McGuire","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"39b15f7e2f8ea9430aca897034f5d0f7","permalink":"/publication/nglod/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/nglod/","section":"publication","summary":" Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics. ","tags":[],"title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Surfaces","type":"publication"},{"authors":["Zan Gojcic","Or Litany","Andreas Wieser","Leonidas J Guibas","Tolga Birdal"],"categories":null,"content":"","date":1618410119,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618410119,"objectID":"378987d1a3e791ce2ae1b5d7a13287de","permalink":"/publication/cvpr_2021_weakly_supervised_rigid/","publishdate":"2021-04-14T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_weakly_supervised_rigid/","section":"publication","summary":"We propose a data-driven scene flow estimation algorithm exploiting the observation that many 3D scenes can be explained by a collection of agents moving as rigid bodies. At the core of our method lies a deep architecture able to reason at the \textbf{object-level} by considering 3D scene flow in conjunction with other 3D tasks. This object level abstraction, enables us to relax the requirement for dense scene flow supervision with simpler binary background segmentation mask and ego-motion annotations. Our mild supervision requirements make our method well suited for recently released massive data collections for autonomous driving, which do not contain dense scene flow annotations. As output, our model provides low-level cues like pointwise flow and higher-level cues such as holistic scene understanding at the level of rigid objects. We further propose a test-time optimization refining the predicted rigid scene flow. We showcase the effectiveness and generalization capacity of our method on four different autonomous driving datasets.","tags":[],"title":"Weakly Supervised Learning of Rigid 3D Scene Flow","type":"publication"},{"authors":["Daiqing Li","Junlin Yang","Karsten Kreis","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618150979,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618150979,"objectID":"dccfda1d1315e0edee11871d6731db61","permalink":"/publication/cvpr_2021_semanticgan/","publishdate":"2021-04-11T10:22:59-04:00","relpermalink":"/publication/cvpr_2021_semanticgan/","section":"publication","summary":"Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semisupervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces.","tags":[],"title":"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization","type":"publication"},{"authors":["He Wang","Yezhen Cong","Or Litany","Yue Gao","Leonidas J. Guibas"],"categories":null,"content":"","date":1618150919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618150919,"objectID":"53a2448442c8c7f2eddf7901c279444a","permalink":"/publication/cvpr_2021_3dioumatch/","publishdate":"2021-04-11T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_3dioumatch/","section":"publication","summary":"3D object detection is an important yet demanding task that heavily relies on difficult to obtain 3D annotations. To reduce the required amount of supervision, we propose 3DIoUMatch, a novel semi-supervised method for 3D object detection applicable to both indoor and outdoor scenes. We leverage a teacher-student mutual learning framework to propagate information from the labeled to the unlabeled train set in the form of pseudo-labels. However, due to the high task complexity, we observe that the pseudo-labels suffer from significant noise and are thus not directly usable. To that end, we introduce a confidence-based filtering mechanism, inspired by FixMatch. We set confidence thresholds based upon the predicted objectness and class probability to filter low-quality pseudo-labels. While effective, we observe that these two measures do not sufficiently capture localization quality. We therefore propose to use the estimated 3D IoU as a localization metric and set category-aware self-adjusted thresholds to filter poorly localized proposals. We adopt VoteNet as our backbone detector on indoor datasets while we use PV-RCNN on the autonomous driving dataset, KITTI. Our method consistently improves state-of-the-art methods on both ScanNet and SUN-RGBD benchmarks by significant margins under all label ratios (including fully labeled setting). For example, when training using only 10% labeled data on ScanNet, 3DIoUMatch achieves 7.7 absolute improvement on mAP@0.25 and 8.5 absolute improvement on mAP@0.5 upon the prior art. On KITTI, we are the first to demonstrate semi-supervised 3D object detection and our method surpasses a fully supervised baseline from 1.8% to 7.6% under different label ratios and categories.","tags":[],"title":"3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection","type":"publication"},{"authors":["Yuxuan Zhang","Wenzheng Chen","Huan Ling","Jun Gao","Yinan Zhang","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1613226179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226179,"objectID":"0a362780443ad68776ee93144d75ce70","permalink":"/publication/iclr_2021_ganverse3d/","publishdate":"2021-02-13T10:22:59-04:00","relpermalink":"/publication/iclr_2021_ganverse3d/","section":"publication","summary":"Differentiable rendering has paved the way to training neural networks to perform 'inverse graphics' tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D 'neural renderer', complementing traditional graphics renderers.","tags":[],"title":"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","type":"publication"},{"authors":["Zhisheng Xiao","Karsten Kreis","Jan Kautz","Arash Vahdat"],"categories":null,"content":"","date":1613226178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226178,"objectID":"129cb72897c577af23deef94768a9736","permalink":"/publication/iclr_2021_-vaebm/","publishdate":"2021-02-13T10:22:58-04:00","relpermalink":"/publication/iclr_2021_-vaebm/","section":"publication","summary":"Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256×256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection.","tags":[],"title":"VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models","type":"publication"},{"authors":["Krishna Murthy Jatavallabhula","Miles Macklin","Florian Golemo","Vikram Voleti","Linda Petrini","Martin Weiss","Breandan Considine","Jérôme Parent-Lévesque","Kevin Xie","Kenny Erleben","Liam Paull","Florian Shkurti","Derek Nowrouzezahrai","Sanja Fidler"],"categories":null,"content":"","date":1613226177,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226177,"objectID":"40b530ba507f7dd3b30c1ec1a3422827","permalink":"/publication/iclr_2021_gradsim/","publishdate":"2021-02-13T10:22:57-04:00","relpermalink":"/publication/iclr_2021_gradsim/","section":"publication","summary":"We consider the problem of estimating an object’s physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph — spanning from the dynamics and through the rendering process — enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.","tags":[],"title":"gradSim: Differentiable simulation for system identification and visuomotor control","type":"publication"},{"authors":["Fei Xia","Chengshu Li","Roberto Martin-Martin","Alexander Toshev","Or Litany","Silvio Savarese"],"categories":null,"content":"","date":1610720519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610720519,"objectID":"2ca38ae9b97a3e641bed54663330f3cc","permalink":"/publication/icra_2021_relmogen/","publishdate":"2021-01-15T10:21:59-04:00","relpermalink":"/publication/icra_2021_relmogen/","section":"publication","summary":"Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator (a combination of motion planner and trajectory executor). We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen -- a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and Hierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots.","tags":[],"title":"ReLMoGen: Integrating Reinforcement Learning and Motion Generation for Interactive Navigation","type":"publication"},{"authors":["Tingwu Wang","Yunrong Guo","Maria Shugrina","Sanja Fidler"],"categories":["Computer Graphics","Computer Vision","Machine Learning","Robotics"],"content":"","date":1602739987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602739987,"objectID":"ad763fd0d6c31a497cf9762fb640649c","permalink":"/publication/unicon/","publishdate":"2020-10-15T01:33:07-04:00","relpermalink":"/publication/unicon/","section":"publication","summary":"The field of physics-based animation is gaining importance due to the increasing demand for realism in video games and films, and has recently seen wide adoption of data-driven techniques, such as deep reinforcement learning (RL), which learn control from (human) demonstrations. While RL has shown impressive results at reproducing individual motions and interactive locomotion, existing methods are limited in their ability to generalize to new motions and their ability to compose a complex motion sequence interactively. In this paper, we propose a physics-based universal neural controller (UniCon) that learns to master thousands of motions with different styles by learning on large-scale motion datasets. UniCon is a two-level framework that consists of a high-level motion scheduler and an RL-powered low-level motion executor, which is our key innovation. By systematically analyzing existing multi-motion RL frameworks, we introduce a novel objective function and training techniques which make a significant leap in performance. Once trained, our motion executor can be combined with different high-level schedulers without the need to retrain, enabling a variety of real-time interactive applications. We show that UniCon can support keyboard-driven control, compose motion sequences drawn from a large pool of locomotion and acrobatics skills and teleport a person captured on video to a physics-based virtual avatar. Numerical and qualitative results demonstrate a significant improvement in efficiency, robustness and generalizability of UniCon over prior state-of-the-art.","tags":["Computer Graphics"],"title":"UniCon: Universal Neural Controller For Physics-based Character Motion","type":"publication"},{"authors":["Krishna Murthy Jatavallabhula","Edward Smith","Jean-Francois Lafleche","Clement Fuji Tsang","Artem Rozantsev","Wenzheng Chen","Tommy Xiang","Rev Lebaredian","Sanja Fidler"],"categories":["Computer Vision"],"content":"News blogpost\nKaolin library documentation\nOmniverse Kaolin app documentation\n","date":1602653587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602653587,"objectID":"b3431fd68aebefba270a45187e318306","permalink":"/publication/kaolin/","publishdate":"2020-10-14T01:33:07-04:00","relpermalink":"/publication/kaolin/","section":"publication","summary":"Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.","tags":["Computer Vision"],"title":"Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research","type":"publication"},{"authors":["Huan Ling","David Acuna","Karsten Kreis","Seung Wook Kim","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1602567187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602567187,"objectID":"f10f6337abaa595062f149d31d2febe6","permalink":"/publication/variational_amodal_object_completion/","publishdate":"2020-10-13T01:33:07-04:00","relpermalink":"/publication/variational_amodal_object_completion/","section":"publication","summary":"","tags":["Computer Vision"],"title":"Variational Amodal Object Completion","type":"publication"},{"authors":["Jun Gao","Wenzheng Chen","Tommy Xiang","Alec Jacobson","Morgan Mcguire","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1601651361,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601651361,"objectID":"95d30d83e35d40520dc38cb60fc1b3a2","permalink":"/publication/def-tet/","publishdate":"2020-10-02T11:09:21-04:00","relpermalink":"/publication/def-tet/","section":"publication","summary":"3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce \u001bmph{Deformable Tetrahedral Meshes} (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTetmatches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D results using only a single image as input.","tags":["Computer Vision","Machine Learning"],"title":"Learning Deformable Tetrahedral Meshes for 3D Reconstruction","type":"publication"},{"authors":["Marc T. Law","Jos Stam"],"categories":["Machine Learning"],"content":"","date":1601616787,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601616787,"objectID":"7831776d5ecda2e923cf15ade5fac0c6","permalink":"/publication/ultrahyperbolic-representation-learning/","publishdate":"2020-10-02T01:33:07-04:00","relpermalink":"/publication/ultrahyperbolic-representation-learning/","section":"publication","summary":"In machine learning, data is usually represented in a (flat) Euclidean space where distances between points are along straight lines. Researchers have recently considered more exotic (non-Euclidean) Riemannian manifolds such as hyperbolic space which is well suited for tree-like data. In this paper, we propose a representation living on a pseudo-Riemannian manifold with constant nonzero curvature. It is a generalization of hyperbolic and spherical geometries where the nondegenerate metric tensor is not positive definite. We provide the necessary learning tools in this geometry and extend gradient method optimization techniques. More specifically, we provide closed-form expressions for distances via geodesics and define a descent direction that guarantees the minimization of the objective problem. Our novel framework is applied to graph representations.","tags":["Machine Learning","Differential Geometry","Optimization"],"title":"Ultrahyperbolic Representation Learning","type":"publication"},{"authors":["Daiqing Li","Amlan Kar","Nishant Ravikumar","Alejandro Frangi","Sanja Fidler"],"categories":["Medical Imaging"],"content":"","date":1601564321,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601564321,"objectID":"33e3d090209d0011de8f815e6b607c53","permalink":"/publication/fed-sim/","publishdate":"2020-10-01T10:58:41-04:00","relpermalink":"/publication/fed-sim/","section":"publication","summary":"Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets.","tags":["Medical Imaging"],"title":"Federated Simulation or Medical Imaging","type":"publication"},{"authors":["Jeevan Devaranjan*","Amlan Kar*","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599283987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599283987,"objectID":"edc9db344e59e5abb984cc595d460904","permalink":"/publication/metasim2/","publishdate":"2020-09-02T01:33:07-04:00","relpermalink":"/publication/metasim2/","section":"publication","summary":"Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods.","tags":["Computer Vision"],"title":"Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation","type":"publication"},{"authors":["Jonah Philion","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599197587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599197587,"objectID":"49eb27543bdebe2edc2645de000f4330","permalink":"/publication/lift-splat-shoot/","publishdate":"2020-09-03T01:33:07-04:00","relpermalink":"/publication/lift-splat-shoot/","section":"publication","summary":"The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single 'bird's-eye-view' coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to 'lift' each image individually into a frustum of features for each camera, then 'splat' all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by 'shooting' template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.","tags":["Computer Vision"],"title":"Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D","type":"publication"},{"authors":["Seung Wook Kim","Yuhao Zhou","Jonah Philion","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1590157319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590157319,"objectID":"b2a8d02ddf839b8af353c1b0940c132c","permalink":"/publication/gamegan/","publishdate":"2020-05-22T10:21:59-04:00","relpermalink":"/publication/gamegan/","section":"publication","summary":"Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN renders the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.","tags":[],"title":"Learning to Simulate Dynamic Environments with GameGAN","type":"publication"},{"authors":["Wenzheng Chen","Jun Gao","Huan Ling","Edward J. Smith","Jaakko Lehtinen","Alec Jacobson","Sanja Fidler"],"categories":null,"content":"","date":1565965319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565965319,"objectID":"655d147143897cd46301c6af3443f28a","permalink":"/publication/dib-r/","publishdate":"2019-08-16T10:21:59-04:00","relpermalink":"/publication/dib-r/","section":"publication","summary":"Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as an distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision.","tags":[],"title":"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer","type":"publication"},{"authors":["Hang Chu","Daiqing Li","David Acuna","Amlan Kar","Maria Shugrina","Xinkai Wei","Ming-Yu Liu","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1564582919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564582919,"objectID":"2eeb7b124f981358ef40bcc65e70b304","permalink":"/publication/ntg/","publishdate":"2019-07-31T10:21:59-04:00","relpermalink":"/publication/ntg/","section":"publication","summary":"We propose Neural Turtle Graphics (NTG), a novel gen- erative model for spatial graphs, and demonstrate its ap- plications in modeling city road layouts. Specifically, we represent the city road layout using a graph where nodes in the graph represent control points and edges in the graph represents segment of roads. NTG is a sequential genera- tive model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train the NTG model on Open Street Map data and show it outperforms ex- isting generative models using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.","tags":[],"title":"Neural Turtle Graphics for Modeling City Road Layouts","type":"publication"},{"authors":["Amlan Kar","Aayush Prakash","Ming-Yu Liu","Eric Cameracci","Justin Yuan","Matt Rusiniak","David Acuna","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1563632519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563632519,"objectID":"4a93c65409127e44851b288b5d8f0278","permalink":"/publication/meta_sim/","publishdate":"2019-07-20T10:21:59-04:00","relpermalink":"/publication/meta_sim/","section":"publication","summary":"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.","tags":[],"title":"Meta Sim: Learning to Generate Synthetic Datasets","type":"publication"},{"authors":["Towaki Takikawa","David Acuna","Varun Jampani","Sanja Fidler"],"categories":null,"content":"","date":1563286919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563286919,"objectID":"bf4d1e11ebe1cb7972e3c1a36197e8c7","permalink":"/publication/gscnn/","publishdate":"2019-07-16T10:21:59-04:00","relpermalink":"/publication/gscnn/","section":"publication","summary":"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. We propose a new architecture that adds a shape stream to the classical CNN architecture. The two streams process the image in parallel, and their information gets fused in the very top layers. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.","tags":[],"title":"Gated-SCNN Gated Shape CNNs for Semantic Segmentation","type":"publication"},{"authors":["David Acuna","Amlan Kar","Sanja Fidler"],"categories":null,"content":"","date":1555424519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555424519,"objectID":"413b78607950e3e5d3b8fd511b19539c","permalink":"/publication/steal/","publishdate":"2019-04-16T10:21:59-04:00","relpermalink":"/publication/steal/","section":"publication","summary":"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.","tags":[],"title":"Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations","type":"publication"}]
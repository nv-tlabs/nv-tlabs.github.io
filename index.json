[{"authors":["sanjafidler"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a008223156da402f6f035b4e37e1bc13","permalink":"/author/sanja-fidler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sanja-fidler/","section":"authors","summary":"","tags":null,"title":"Sanja Fidler","type":"authors"},{"authors":["davidacunamarrero"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"052a025ce9438f4d0241f93c9d69c255","permalink":"/author/david-acuna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-acuna/","section":"authors","summary":"","tags":null,"title":"David Acuna","type":"authors"},{"authors":["matanatzmon"],"categories":null,"content":"I am a PhD student at the Weizmann Institute of Science. My research focuses on devising 3D deep learning methods, mostly interested in learning with weak supervision, and 3D generative models.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"053495656778fd3f66b2df0c6e6c020d","permalink":"/author/matan-atzmon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matan-atzmon/","section":"authors","summary":"I am a PhD student at the Weizmann Institute of Science. My research focuses on devising 3D deep learning methods, mostly interested in learning with weak supervision, and 3D generative models.","tags":null,"title":"Matan Atzmon","type":"authors"},{"authors":["souravbiswas"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA focusing on improving graphics applications with deep learning. I\u0026rsquo;m currently an undergraduate student at the University of Waterloo studying computer science and finance. My main focus is computer vision and data privacy.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"550f894459d6ab8ad4af3c16e8ee662e","permalink":"/author/sourav-biswas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sourav-biswas/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA focusing on improving graphics applications with deep learning. I\u0026rsquo;m currently an undergraduate student at the University of Waterloo studying computer science and finance. My main focus is computer vision and data privacy.","tags":null,"title":"Sourav Biswas","type":"authors"},{"authors":["bradleybrown"],"categories":null,"content":"I\u0026rsquo;m an undergraduate student at the University of Waterloo, studying software engineering with a minor in combinatorics and optimization. At NVIDIA, I work as a research intern focusing on 3D generative models and neural simulators.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"37382a17d7aef3d50dedb35eb76700c8","permalink":"/author/bradley-brown/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bradley-brown/","section":"authors","summary":"I\u0026rsquo;m an undergraduate student at the University of Waterloo, studying software engineering with a minor in combinatorics and optimization. At NVIDIA, I work as a research intern focusing on 3D generative models and neural simulators.","tags":null,"title":"Bradley Brown","type":"authors"},{"authors":["tianshicao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"22fab2016a26de835b2c60c65dd3c6ce","permalink":"/author/tianshi-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianshi-cao/","section":"authors","summary":"","tags":null,"title":"Tianshi Cao","type":"authors"},{"authors":["bowenchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"46eb5d964c2c33917c1f46d49f219e44","permalink":"/author/bowen-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bowen-chen/","section":"authors","summary":"","tags":null,"title":"Bowen Chen","type":"authors"},{"authors":["wenzhengcheng"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d4aab82c8b2f97308fd9b03061baa62","permalink":"/author/wenzheng-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenzheng-chen/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Wenzheng Chen","type":"authors"},{"authors":["zhiqinchen"],"categories":null,"content":"Zhiqin Chen is a 2nd year Ph.D. student at Simon Fraser University, under the supervision of Prof. Hao (Richard) Zhang. His research interest is Computer Graphics with a specialty in Geometric Modeling and Machine Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aaa73a9a79978c24305e6029d636c0c0","permalink":"/author/zhiqin-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiqin-chen/","section":"authors","summary":"Zhiqin Chen is a 2nd year Ph.D. student at Simon Fraser University, under the supervision of Prof. Hao (Richard) Zhang. His research interest is Computer Graphics with a specialty in Geometric Modeling and Machine Learning.","tags":null,"title":"Zhiqin Chen","type":"authors"},{"authors":["clementfujitsang"],"categories":null,"content":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c2429b6ac12c5dc84df0223e7517c65","permalink":"/author/clement-fuji-tsang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/clement-fuji-tsang/","section":"authors","summary":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.","tags":null,"title":"Clement Fuji Tsang","type":"authors"},{"authors":["jungao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4290fc14b9975f84c3d7f082f170356","permalink":"/author/jun-gao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jun-gao/","section":"authors","summary":"","tags":null,"title":"Jun Gao","type":"authors"},{"authors":["zangojcic"],"categories":null,"content":"I am a research scientist at NVIDIA. I am broadly interested in general 3D vision problems. During my Ph.D. my research has mostly focused on incorporating a local rigidity inductive bias into models for point cloud registration and scene flow estimation.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e07d7147e171cebad490a9010f911403","permalink":"/author/zan-gojcic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zan-gojcic/","section":"authors","summary":"I am a research scientist at NVIDIA. I am broadly interested in general 3D vision problems. During my Ph.D. my research has mostly focused on incorporating a local rigidity inductive bias into models for point cloud registration and scene flow estimation.","tags":null,"title":"Zan Gojcic","type":"authors"},{"authors":["timdockhorn"],"categories":null,"content":"I am a PhD student at the University of Waterloo working on a variety of topics in machine learning. In my internship I am working on improving large generative models by using fundamental ideas from physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ebc05bd7317ed4f991addb6d713f4790","permalink":"/author/tim-dockhorn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tim-dockhorn/","section":"authors","summary":"I am a PhD student at the University of Waterloo working on a variety of topics in machine learning. In my internship I am working on improving large generative models by using fundamental ideas from physics.","tags":null,"title":"Tim Dockhorn","type":"authors"},{"authors":["amlankar"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cc1152b762fdb5eb7959a8087610fe02","permalink":"/author/amlan-kar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amlan-kar/","section":"authors","summary":"","tags":null,"title":"Amlan Kar","type":"authors"},{"authors":["mohamedhassan"],"categories":null,"content":"I am a Ph.D. student at Max Planck Institute for Intelligent Systems, where I work on computer vision, computer graphics and machine learning. I am advised by Michael Black. I am interested in reconstructing, analyzing, and generating Human-Scene Interaction.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"311ad85f1f2e978f23c2b8c103ea9cf3","permalink":"/author/mohamed-hassan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mohamed-hassan/","section":"authors","summary":"I am a Ph.D. student at Max Planck Institute for Intelligent Systems, where I work on computer vision, computer graphics and machine learning. I am advised by Michael Black. I am interested in reconstructing, analyzing, and generating Human-Scene Interaction.","tags":null,"title":"Mohamed Hassan","type":"authors"},{"authors":["samehkhamis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"732c7bd8ae9bcb1d929f34f6620da459","permalink":"/author/sameh-khamis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sameh-khamis/","section":"authors","summary":"","tags":null,"title":"Sameh Khamis","type":"authors"},{"authors":["seungwookkim"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"594015d730259c3a92f033305a532e2f","permalink":"/author/seung-wook-kim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/seung-wook-kim/","section":"authors","summary":"","tags":null,"title":"Seung Wook Kim","type":"authors"},{"authors":["karstenkreis"],"categories":null,"content":"I am a Senior Research Scientist at NVIDIA’s Toronto AI Lab. I am trained as a physicist and completed my master’s in quantum information theory. For my Ph.D. in computational and statistical physics, I developed multiscale models and sampling algorithms for molecular dynamics simulations of complex chemical and biological systems. After I finished my Ph.D., I switched to deep learning. Before joining NVIDIA, I worked on deep generative modeling at D-Wave Systems, a quantum computation company, and I co-founded Variational AI, a startup focusing on generative modeling for drug discovery.\nI have always been excited by developing mathematical frameworks and algorithmic and data-driven approaches to simulate and model our physical world and to synthesize novel but realistic data from scratch. Currently, my primary research interests revolve around deep generative models. I am interested both in fundamental algorithm development and in applying these models on relevant problems in areas such as representation learning, computer vision, graphics and digital artistry. I am also broadly interested in research that takes inspirations from physics to improve machine learning techniques as well as in applying state-of-the-art deep learning methods to problems in the natural sciences.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a216c907a09258df8f398909408d130a","permalink":"/author/karsten-kreis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/karsten-kreis/","section":"authors","summary":"I am a Senior Research Scientist at NVIDIA’s Toronto AI Lab. I am trained as a physicist and completed my master’s in quantum information theory. For my Ph.D. in computational and statistical physics, I developed multiscale models and sampling algorithms for molecular dynamics simulations of complex chemical and biological systems.","tags":null,"title":"Karsten Kreis","type":"authors"},{"authors":["aliceli"],"categories":null,"content":"I am Alice Li, who’ll be entering year 4 at the University of Toronto. My research interests lie at the intersection of computer vision and computer graphics. A physicist by training, I found ray-tracing/rendering/GPGPU interesting as well.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f08fd6f1d99497eec249268be4d14594","permalink":"/author/alice-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alice-li/","section":"authors","summary":"I am Alice Li, who’ll be entering year 4 at the University of Toronto. My research interests lie at the intersection of computer vision and computer graphics. A physicist by training, I found ray-tracing/rendering/GPGPU interesting as well.","tags":null,"title":"Alice Li","type":"authors"},{"authors":["marclaw"],"categories":null,"content":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks. More specifically, my domains of interest are: distance metric learning, complex representations including hierarchies and graphs, convex and non-convex optimization, optimization on manifolds, information geometry, structured output prediction, generalization with limited supervision.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f02ed7ceb48db52a1c5142bb4607614b","permalink":"/author/marc-law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marc-law/","section":"authors","summary":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks.","tags":null,"title":"Marc Law","type":"authors"},{"authors":["michaelli"],"categories":null,"content":"I am a research engineer intern working on Kaolin. Broadly, I am interested in applying research findings to real world problems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d283b77e1295e97ba63f721cba4a4728","permalink":"/author/michael-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-li/","section":"authors","summary":"I am a research engineer intern working on Kaolin. Broadly, I am interested in applying research findings to real world problems.","tags":null,"title":"Michael Li","type":"authors"},{"authors":["derekliu"],"categories":null,"content":"Hsueh-Ti Derek Liu is a Ph.D. student at the University of Toronto supervised by Alec Jacobson. Derek received his B.S.E. at National Taiwan University, and M.S. at Carnegie Mellon University advised by Keenan Crane and Levent Burak Kara. Derek\u0026rsquo;s research focuses on digital geometry processing. He aims at developing easy-to-use tools for 3D content creation and algorithms for processing geometric data at scale. Derek\u0026rsquo;s research is partially supported by Adobe Research Fellowship and the Mary H. Beatty Fellowship. Recently, he is doing an internship at NVIDIA mentored by Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57eb045cf7de2b7c6ffb7240f52d8cb6","permalink":"/author/derek-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/derek-liu/","section":"authors","summary":"Hsueh-Ti Derek Liu is a Ph.D. student at the University of Toronto supervised by Alec Jacobson. Derek received his B.S.E. at National Taiwan University, and M.S. at Carnegie Mellon University advised by Keenan Crane and Levent Burak Kara.","tags":null,"title":"Derek Liu","type":"authors"},{"authors":["daiqingli"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ee0d6dbbf827a332ae71729722511b7a","permalink":"/author/daiqing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daiqing-li/","section":"authors","summary":"","tags":null,"title":"Daiqing Li","type":"authors"},{"authors":["jameslucas"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD candidate at the University of Toronto. My research broadly works towards a better understanding of deep learning and the optimization of neural networks. At NVIDIA, I\u0026rsquo;m developing methods to improve deep learning models in data-scarce settings and working with generative models of 3D data.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84b15291b5248c5e6cdeddee7ae1a3e6","permalink":"/author/james-lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/james-lucas/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD candidate at the University of Toronto. My research broadly works towards a better understanding of deep learning and the optimization of neural networks.","tags":null,"title":"James Lucas","type":"authors"},{"authors":["huanling"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eb3fc86e8c4373b23194a1d7a96d5481","permalink":"/author/huan-ling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huan-ling/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Huan Ling","type":"authors"},{"authors":["jasonpeng"],"categories":null,"content":"I\u0026rsquo;m currently a Ph.D. student at UC Berkeley advised by Professor Sergey Levine and Professor Pieter Abbeel. I received an M.Sc from the University of British Columbia, advised by Professor Michiel van de Panne. My work lies in the intersection between computer graphics and machine learning, with a focus on reinforcement learning for motion control of simulated characters.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c41fc4ac8c4ce6e34bf21a5d9edbbd51","permalink":"/author/jason-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jason-peng/","section":"authors","summary":"I\u0026rsquo;m currently a Ph.D. student at UC Berkeley advised by Professor Sergey Levine and Professor Pieter Abbeel. I received an M.Sc from the University of British Columbia, advised by Professor Michiel van de Panne.","tags":null,"title":"Jason Peng","type":"authors"},{"authors":["joeylitalien"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"58cac3c6accbb4f342c3d19de02b3e2c","permalink":"/author/joey-litalien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/joey-litalien/","section":"authors","summary":"","tags":null,"title":"Joey Litalien","type":"authors"},{"authors":["orlitany"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"446ebc3097a2014a6dac95a5dcfe857c","permalink":"/author/or-litany/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/or-litany/","section":"authors","summary":"","tags":null,"title":"Or Litany","type":"authors"},{"authors":["davisrempe"],"categories":null,"content":"I am a PhD student at Stanford University advised by Prof. Leonidas Guibas. My research focuses on leveraging learned and physics-based motion models to improve 3D perception and synthesis of dynamic objects and humans.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b0e39f164d5da208e93d3678590967e7","permalink":"/author/davis-rempe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/davis-rempe/","section":"authors","summary":"I am a PhD student at Stanford University advised by Prof. Leonidas Guibas. My research focuses on leveraging learned and physics-based motion models to improve 3D perception and synthesis of dynamic objects and humans.","tags":null,"title":"Davis Rempe","type":"authors"},{"authors":["rafidmahmood"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b4ba619dd5ed502941db478c875271c","permalink":"/author/rafid-mahmood/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rafid-mahmood/","section":"authors","summary":"","tags":null,"title":"Rafid Mahmood","type":"authors"},{"authors":["despoinapaschalidou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"71062a171cd9dcbb0030b96d536ffd19","permalink":"/author/despoina-paschalidou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/despoina-paschalidou/","section":"authors","summary":"","tags":null,"title":"Despoina Paschalidou","type":"authors"},{"authors":["jonahphilion"],"categories":null,"content":"Website\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d881c29c284c9572f14b5f49c6c6d13e","permalink":"/author/jonah-philion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonah-philion/","section":"authors","summary":"Website","tags":null,"title":"Jonah Philion","type":"authors"},{"authors":["gopalsharma"],"categories":null,"content":"I am a Ph.D. student at CICS UMass-Amherst. I am advised by Prof. Evangelos Kalogerakis, and Prof. Subhransu Maji. I work on learning interpretable and editable representation of shapes using neural networks. I also work on utilizing geometry for self-supervised representation learning for 3D shapes.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb609d3ad687961bd3b4d08eea3b24e0","permalink":"/author/gopal-sharma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/gopal-sharma/","section":"authors","summary":"I am a Ph.D. student at CICS UMass-Amherst. I am advised by Prof. Evangelos Kalogerakis, and Prof. Subhransu Maji. I work on learning interpretable and editable representation of shapes using neural networks.","tags":null,"title":"Gopal Sharma","type":"authors"},{"authors":["cinjonresnick"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7004af2e6a5b0c3f3b2131552c2cc1b8","permalink":"/author/cinjon-resnick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cinjon-resnick/","section":"authors","summary":"","tags":null,"title":"Cinjon Resnick","type":"authors"},{"authors":["frankshen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3dd516552086188aec3e991458f3bf5","permalink":"/author/frank-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/frank-shen/","section":"authors","summary":"","tags":null,"title":"Frank Shen","type":"authors"},{"authors":["aotang"],"categories":null,"content":"I am a research engineer intern working on Kaolin and Nvidia AI Playground. I am interested in 3D computer vision area as well as physics based simulation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb5559d71dd103172537f2204bb19334","permalink":"/author/ao-tang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ao-tang/","section":"authors","summary":"I am a research engineer intern working on Kaolin and Nvidia AI Playground. I am interested in 3D computer vision area as well as physics based simulation.","tags":null,"title":"Ao Tang","type":"authors"},{"authors":["mashashugrina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b710e53d7696c37bd5f9e1c5b7f4147","permalink":"/author/masha-shugrina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/masha-shugrina/","section":"authors","summary":"","tags":null,"title":"Masha Shugrina","type":"authors"},{"authors":["zianwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ff622ef7205d9df98598e14b66b9609","permalink":"/author/zian-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zian-wang/","section":"authors","summary":"","tags":null,"title":"Zian Wang","type":"authors"},{"authors":["towakitakikawa"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ce5d6218d34a263e6c634c625045dfa4","permalink":"/author/towaki-takikawa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/towaki-takikawa/","section":"authors","summary":"","tags":null,"title":"Towaki Takikawa","type":"authors"},{"authors":["tommyxiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ba5ce1113200696d0ce4a55f1555f978","permalink":"/author/tommy-xiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tommy-xiang/","section":"authors","summary":"","tags":null,"title":"Tommy Xiang","type":"authors"},{"authors":["tingwuwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"00033e991e190cf6e4a354975f38e86d","permalink":"/author/tingwu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tingwu-wang/","section":"authors","summary":"","tags":null,"title":"Tingwu Wang","type":"authors"},{"authors":["franciswilliams"],"categories":null,"content":"I am a research scientist at NVIDIA in NYC working at the intersection of computer vision, machine learning, and computer graphics. My research is a mix of theory and application, aiming to solve practical problems in elegant ways. In particular, I’m very interested in 3D shape representations which can enable deep learning on “real-world” geometric datasets which are often noisy, unlabeled, and consisting of very large inputs.\nI completed my PhD from NYU in 2021 where I worked in the Math and Data Group and the Geometric Computing Lab. My advisors were Joan Bruna and Denis Zorin.\nIn addition to research, I also develop, maintain, and contribute to several open source projects. These include NumpyEigen, Point Cloud Utils, and FML.\nI’m currently looking for motivated interns to work with. Please reach out to me if you would like to chat about potential collaborations at NVIDIA!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"022d548e63965a209e867ffdb0125b1d","permalink":"/author/francis-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/francis-williams/","section":"authors","summary":"I am a research scientist at NVIDIA in NYC working at the intersection of computer vision, machine learning, and computer graphics. My research is a mix of theory and application, aiming to solve practical problems in elegant ways.","tags":null,"title":"Francis Williams","type":"authors"},{"authors":["kevinxie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b273d4708ae30cdc04370ea1a5d7ace7","permalink":"/author/kevin-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kevin-xie/","section":"authors","summary":"","tags":null,"title":"Kevin Xie","type":"authors"},{"authors":["junlinyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d45085dc6bc342fd9b9bcd6ee2ed7c62","permalink":"/author/junlin-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junlin-yang/","section":"authors","summary":"","tags":null,"title":"Junlin Yang","type":"authors"},{"authors":["kangxueyin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"32703f897bd8424fc595e7af09cf9d14","permalink":"/author/kangxue-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kangxue-yin/","section":"authors","summary":"","tags":null,"title":"Kangxue Yin","type":"authors"},{"authors":["xiaohuizeng"],"categories":null,"content":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD student at the University of Toronto. I am interested in generative model on 2D image and 3D objects. At NVIDIA, I\u0026rsquo;m working with 3D shapes generation.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"23229d34f714feca698deed587348fbb","permalink":"/author/xiaohui-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaohui-zeng/","section":"authors","summary":"I\u0026rsquo;m a research intern at NVIDIA, and a PhD student at the University of Toronto. I am interested in generative model on 2D image and 3D objects. At NVIDIA, I\u0026rsquo;m working with 3D shapes generation.","tags":null,"title":"Xiaohui Zeng","type":"authors"},{"authors":["alexzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"df402183d6954aec8a0979fda4aac6ab","permalink":"/author/alex-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-zhang/","section":"authors","summary":"","tags":null,"title":"Alex Zhang","type":"authors"},{"authors":["siyanzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0721bfdb7795b0de16a0ba459c4719ae","permalink":"/author/siyan-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siyan-zhao/","section":"authors","summary":"","tags":null,"title":"Siyan Zhao","type":"authors"},{"authors":["adityakusupati"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b61a279523f54138e79b739ad678a79","permalink":"/author/aditya-kusupati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aditya-kusupati/","section":"authors","summary":"","tags":null,"title":"Aditya Kusupati","type":"authors"},{"authors":["alexbie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fcb08531a5bf13e454f0ff379c929ea9","permalink":"/author/alex-bie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-bie/","section":"authors","summary":"","tags":null,"title":"Alex Bie","type":"authors"},{"authors":["guojunzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"97d79e79ff77e9981453b8f24aba6fcf","permalink":"/author/guojun-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guojun-zhang/","section":"authors","summary":"","tags":null,"title":"Guojun Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics. Our group members are also part of or closely collaborate with academic labs such as the University of Toronto and Vector Institute.\nWe invite applications for the following positions:\n full time research scientist full time research engineer research scientist intern research engineer intern  See this link for open positions, or contact our members for more details.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/toronto-ai-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/toronto-ai-lab/","section":"authors","summary":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics.","tags":null,"title":"Toronto AI Lab","type":"authors"},{"authors":["Despoina Paschalidou","Amlan Kar","Maria Shugrina","Karsten Kreis","Andreas Geiger","Sanja Fidler"],"categories":null,"content":"","date":1635780239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635780239,"objectID":"43866d6323f07bbecc1c8e5a0f5054e1","permalink":"/publication/neurips_2021_atiss/","publishdate":"2021-11-01T11:23:59-04:00","relpermalink":"/publication/neurips_2021_atiss/","section":"publication","summary":"The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.","tags":[],"title":"ATISS: Autoregressive Transformers for Indoor Scene Synthesis","type":"publication"},{"authors":["Arash Vahdat","Karsten Kreis","Jan Kautz"],"categories":null,"content":"","date":1635776639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635776639,"objectID":"333cbbee33cb9be334c742d633b08966","permalink":"/publication/neurips_2021_lsgm/","publishdate":"2021-11-01T10:23:59-04:00","relpermalink":"/publication/neurips_2021_lsgm/","section":"publication","summary":"Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset.","tags":[],"title":"Score-based Generative Modeling in Latent Space","type":"publication"},{"authors":["David Acuna","Jonah Philion","Sanja Fidler"],"categories":null,"content":"","date":1635690239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690239,"objectID":"14be48224b099ab05129e8982e585599","permalink":"/publication/neurips_2021_simulation_strategies/","publishdate":"2021-10-31T10:23:59-04:00","relpermalink":"/publication/neurips_2021_simulation_strategies/","section":"publication","summary":"Autonomous driving relies on a huge volume of real-world data to be labeled to high precision. Alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations. However, the domain gap between the synthetic and real data remains, raising the following important question: What are the best ways to utilize a self-driving simulator for perception tasks? In this work, we build on top of recent advances in domain-adaptation theory, and from this perspective, propose ways to minimize the reality gap. We primarily focus on the use of labels in the synthetic domain alone. Our approach introduces both a principled way to learn neural-invariant representations and a theoretically inspired view on how to sample the data from the simulator. Our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator. We showcase our approach on the bird's-eye-view vehicle segmentation task with multi-sensor data (cameras, lidar) using an open-source simulator (CARLA), and evaluate the entire framework on a real-world dataset (nuScenes). Last but not least, we show what types of variations (e.g. weather conditions, number of assets, map design, and color diversity) matter to perception networks when trained with driving simulators, and which ones can be compensated for with our domain adaptation technique.","tags":[],"title":"Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation","type":"publication"},{"authors":["Marc T. Law"],"categories":null,"content":"","date":1635690179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690179,"objectID":"3a7f979069086d8dee92cd0483bf0578","permalink":"/publication/neurips_2021_ultrahyperbolic/","publishdate":"2021-10-31T10:22:59-04:00","relpermalink":"/publication/neurips_2021_ultrahyperbolic/","section":"publication","summary":"Riemannian space forms, such as the Euclidean space, sphere and hyperbolic space, are popular and powerful representation spaces in machine learning. For instance, hyperbolic geometry is appropriate to represent graphs without cycles and has been used to extend Graph Neural Networks. Recently, some pseudo-Riemannian space forms that generalize both hyperbolic and spherical geometries have been exploited to learn a specific type of nonparametric embedding called ultrahyperbolic. The lack of geodesic between every pair of ultrahyperbolic points makes the task of learning parametric models (e.g., neural networks) difficult. This paper introduces a method to learn parametric models in ultrahyperbolic space. We experimentally show the relevance of our approach in the tasks of graph and node classification. ","tags":[],"title":"Ultrahyperbolic Neural Networks","type":"publication"},{"authors":["Wenzheng Chen","Joey Litalien","Jun Gao","Zian Wang","Clement Fuji Tsang","Sameh Khamis","Or Litany","Sanja Fidler"],"categories":null,"content":"","date":1635690178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690178,"objectID":"d57d99f1d471875897298760ff18273b","permalink":"/publication/neurips_2021_dibr/","publishdate":"2021-10-31T10:22:58-04:00","relpermalink":"/publication/neurips_2021_dibr/","section":"publication","summary":"We consider the challenging problem of predicting intrinsic object properties from a single image by exploiting differentiable renderers. Many previous learning-based approaches for inverse graphics adopt rasterization-based renderers and assume naive lighting and material models, which often fail to account for non-Lambertian, specular reflections commonly observed in the wild. In this work, we propose DIBR++, a hybrid differentiable renderer which supports these photorealistic effects by combining rasterization and ray-tracing, taking the advantage of their respective strengths -- speed and realism. Our renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. Compared to more advanced physics-based differentiable renderers leveraging path tracing, DIBR++ is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. We experimentally demonstrate that our approach achieves superior material and lighting disentanglement on synthetic and real data compared to existing rasterization-based approaches and showcase several artistic applications including material editing and relighting.","tags":[],"title":"DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer","type":"publication"},{"authors":["Tianshi Cao","Alex Bie","Arash Vahdat","Sanja Fidler","Karsten Kreis"],"categories":null,"content":"","date":1635690119,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690119,"objectID":"11f5c139d27bd9ea9f4423a34ce2619b","permalink":"/publication/neurips_2021_differentially_private_generative/","publishdate":"2021-10-31T10:21:59-04:00","relpermalink":"/publication/neurips_2021_differentially_private_generative/","section":"publication","summary":"Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.","tags":[],"title":"Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence","type":"publication"},{"authors":["Huan Ling","Karsten Kreis","Daiqing Li","Seung Wook Kim","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1635690118,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635690118,"objectID":"f5c97e409a5fd474d72821bd806e3a27","permalink":"/publication/neurips_2021_editgan/","publishdate":"2021-10-31T10:21:58-04:00","relpermalink":"/publication/neurips_2021_editgan/","section":"publication","summary":"Generative adversarial networks (GANs) have recently found applications in image editing. However, most GAN based image editing methods often require large scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high quality, high precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car. EditGAN builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. Specifically, we embed an image into the GAN latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, we find editing vectors in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. We demonstrate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks.","tags":[],"title":"EditGAN: High-Precision Semantic Image Editing","type":"publication"},{"authors":["Zian Wang","Jonah Philion","Sanja Fidler","Jan Kautz"],"categories":null,"content":"","date":1630419719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630419719,"objectID":"25e5d05b1a9c7e8c168ba467ed777744","permalink":"/publication/iccv_2021_inverse_rendering/","publishdate":"2021-08-31T10:21:59-04:00","relpermalink":"/publication/iccv_2021_inverse_rendering/","section":"publication","summary":"In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.","tags":[],"title":"Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting","type":"publication"},{"authors":["Kangxue Yin","Jun Gao","Maria Shugrina","Sameh Khamis","Sanja Fidler"],"categories":null,"content":"","date":1630333319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630333319,"objectID":"61127a945d849ad83b2cf32becbaa148","permalink":"/publication/iccv_2021_3dstylenet/","publishdate":"2021-08-30T10:21:59-04:00","relpermalink":"/publication/iccv_2021_3dstylenet/","section":"publication","summary":"We propose a method to create plausible geometric and texture style variations of 3D objects in the quest to democratize 3D content creation. Given a pair of textured source and target objects, our method predicts a part-aware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target. In addition, the texture style of the target is transferred to the warped source object with the help of a multi-view differentiable renderer. Our model, 3DStyleNet, is composed of two sub-networks trained in two stages. First, the geometric style network is trained on a large set of untextured 3D shapes. Second, we jointly optimize our geometric style network and a pre-trained image style transfer network with losses defined over both the geometry and the rendering of the result. Given a small set of high-quality textured objects, our method can create many novel stylized shapes, resulting in effortless 3D content creation and style-ware data augmentation. We showcase our approach qualitatively on 3D content stylization, and provide user studies to validate the quality of our results. In addition, our method can serve as a valuable tool to create 3D data augmentations for computer vision tasks. Extensive quantitative analysis shows that 3DStyleNet outperforms alternative data augmentation techniques for the downstream task of single-image 3D reconstruction.","tags":[],"title":"3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations","type":"publication"},{"authors":["Congyue Deng","Or Litany","Yueqi Duan","Adrien Poulenard","Andrea Tagliasacchi","Leonidas Guibas"],"categories":null,"content":"","date":1630246919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630246919,"objectID":"9b64908c712ea0bbd2b4b2684ba269e9","permalink":"/publication/iccv_2021_vector_neurons/","publishdate":"2021-08-29T10:21:59-04:00","relpermalink":"/publication/iccv_2021_vector_neurons/","section":"publication","summary":"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.","tags":[],"title":"Vector Neurons: A General Framework for SO(3)-Equivariant Networks","type":"publication"},{"authors":["Kevin Xie","Tingwu Wang","Umar Iqbal","Yunrong Guo","Sanja Fidler","Florian Shkurti"],"categories":null,"content":"","date":1630246918,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630246918,"objectID":"52f3e09fda74a1b3327178e3aceaca94","permalink":"/publication/iccv_2021_physics/","publishdate":"2021-08-29T10:21:58-04:00","relpermalink":"/publication/iccv_2021_physics/","section":"publication","summary":"Human motion synthesis is an important problem with applications in graphics, gaming and simulation environments for robotics. Existing methods require accurate motion capture data for training, which is costly to obtain. Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely available. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corresponding contact forces. Results show that our physically-corrected motions significantly outperform prior work on pose estimation. We can then use these to train a generative model to synthesize future motion. Samples from the generative model can optionally be further refined with the same physics correction optimization. We demonstrate both qualitatively and quantitatively significantly improved motion estimation, synthesis quality and physical plausibility achieved by our method on the large scale Human3.6m dataset as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, realistic and diverse motion synthesis.","tags":[],"title":"Physics-based Human Motion Estimation and Synthesis from Videos","type":"publication"},{"authors":["Aayush Prakash","Shoubhik Debnath","Jean-Francois Lafleche","Eric Cameracci","Gavriel State","Stan Birchfield","Marc T. Law"],"categories":null,"content":"","date":1629555719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629555719,"objectID":"1fc4de8c03f3a7942bcc8f8be8e45adc","permalink":"/publication/iccv_2021_sim2sg/","publishdate":"2021-08-21T10:21:59-04:00","relpermalink":"/publication/iccv_2021_sim2sg/","section":"publication","summary":"Synthetic data is emerging as a promising solution to the scalability issue of supervised deep learning, especially when real data are difficult to acquire or hard to annotate. Synthetic data generation, however, can itself be prohibitively expensive when domain experts have to manually and painstakingly oversee the process. Moreover, neural networks trained on synthetic data often do not perform well on real data because of the domain gap. To solve these challenges, we propose Sim2SG, a self-supervised automatic scene generation technique for matching the distribution of real data. Importantly, Sim2SG does not require supervision from the real-world dataset, thus making it applicable in situations for which such annotations are difficult to obtain. Sim2SG is designed to bridge both the content and appearance gaps, by matching the content of real data, and by matching the features in the source and target domains. We select scene graph (SG) generation as the downstream task, due to the limited availability of labeled datasets. Experiments demonstrate significant improvements over leading baselines in reducing the domain gap both qualitatively and quantitatively, on several synthetic datasets as well as the real-world KITTI dataset.","tags":[],"title":"Self-Supervised Real-to-Sim Scene Generation","type":"publication"},{"authors":["David Acuna","Guojun Zhang","Marc T. Law","Sanja Fidler"],"categories":null,"content":"","date":1626099779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626099779,"objectID":"337e18597ffa2992824e3cf74d5ca9cf","permalink":"/publication/icml_2021_fdal/","publishdate":"2021-07-12T10:22:59-04:00","relpermalink":"/publication/icml_2021_fdal/","section":"publication","summary":"Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a) as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines, and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.","tags":[],"title":"f-Domain-Adversarial Learning: Theory and Algorithms","type":"publication"},{"authors":["Nadine Chang","Zhiding Yu","Yu-Xiong Wang","Anima Anandkumar","Sanja Fidler","Jose M. Alvarez"],"categories":null,"content":"","date":1626096179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626096179,"objectID":"edff1664ff7f4731ddca083a074c44f5","permalink":"/publication/icml_2021_long_tail/","publishdate":"2021-07-12T10:22:59-03:00","relpermalink":"/publication/icml_2021_long_tail/","section":"publication","summary":"Training on datasets with long-tailed distributions has been challenging for major recognition tasks such as classification and detection. To deal with this challenge, image resampling is typically introduced as a simple but effective approach. However, we observe that long-tailed detection differs from classification since multiple classes may be present in one image. As a result, image resampling alone is not enough to yield a sufficiently balanced distribution at the object level. We address object-level resampling by introducing an object-centric memory replay strategy based on dynamic, episodic memory banks. Our proposed strategy has two benefits: 1) convenient object-level resampling without significant extra computation, and 2) implicit feature-level augmentation from model updates. We show that image-level and object-level resamplings are both important, and thus unify them with a joint resampling strategy (RIO). Our method outperforms state-of-the-art long-tailed detection and segmentation methods on LVIS v0.5 across various backbones.","tags":[],"title":"Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection","type":"publication"},{"authors":["Yuxuan Zhang","Huan Ling","Jun Gao","Kangxue Yin","Jean-Francois Lafleche","Adela Barriuso","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"945e62e056e2517eef12fae1051ad8f1","permalink":"/publication/datasetgan/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/datasetgan/","section":"publication","summary":"We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods using labor intensive annotations.","tags":[],"title":"DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort","type":"publication"},{"authors":["Seung Wook Kim","Jonah Philion","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"89a313283a90b62755e1d9e58392b441","permalink":"/publication/cvpr_2021_drivegan/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_drivegan/","section":"publication","summary":"Realistic simulators are critical for training and verifying robotics systems. While most of the contemporary simulators are hand-crafted, a scaleable way to build simulators is to use machine learning to learn how the environment behaves in response to an action, directly from data. In this work, we aim to learn to simulate a dynamic environment directly in pixel-space, by watching unannotated sequences of frames and their associated action pairs. We introduce a novel high-quality neural simulator referred to as DriveGAN that achieves controllability by disentangling different components without supervision. In addition to steering controls, it also includes controls for sampling features of a scene, such as the weather as well as the location of non-player objects. Since DriveGAN is a fully differentiable simulator, it further allows for re-simulation of a given video sequence, offering an agent to drive through a recorded scene again, possibly taking different actions. We train DriveGAN on multiple datasets, including 160 hours of real-world driving data. We showcase that our approach greatly surpasses the performance of previous data-driven simulators, and allows for new features not explored before.","tags":[],"title":"DriveGAN: Towards a Controllable High-Quality Neural Simulation","type":"publication"},{"authors":["Towaki Takikawa","Joey Litalien","Kangxue Yin","Karsten Kreis","Charles Loop","Derek Nowrouzezahrai","Alec Jacobson","Morgan McGuire","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"39b15f7e2f8ea9430aca897034f5d0f7","permalink":"/publication/nglod/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/nglod/","section":"publication","summary":" Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics. ","tags":[],"title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Surfaces","type":"publication"},{"authors":["Zan Gojcic","Or Litany","Andreas Wieser","Leonidas J Guibas","Tolga Birdal"],"categories":null,"content":"","date":1618410119,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618410119,"objectID":"378987d1a3e791ce2ae1b5d7a13287de","permalink":"/publication/cvpr_2021_weakly_supervised_rigid/","publishdate":"2021-04-14T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_weakly_supervised_rigid/","section":"publication","summary":"We propose a data-driven scene flow estimation algorithm exploiting the observation that many 3D scenes can be explained by a collection of agents moving as rigid bodies. At the core of our method lies a deep architecture able to reason at the \textbf{object-level} by considering 3D scene flow in conjunction with other 3D tasks. This object level abstraction, enables us to relax the requirement for dense scene flow supervision with simpler binary background segmentation mask and ego-motion annotations. Our mild supervision requirements make our method well suited for recently released massive data collections for autonomous driving, which do not contain dense scene flow annotations. As output, our model provides low-level cues like pointwise flow and higher-level cues such as holistic scene understanding at the level of rigid objects. We further propose a test-time optimization refining the predicted rigid scene flow. We showcase the effectiveness and generalization capacity of our method on four different autonomous driving datasets.","tags":[],"title":"Weakly Supervised Learning of Rigid 3D Scene Flow","type":"publication"},{"authors":["Daiqing Li","Junlin Yang","Karsten Kreis","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618150979,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618150979,"objectID":"dccfda1d1315e0edee11871d6731db61","permalink":"/publication/cvpr_2021_semanticgan/","publishdate":"2021-04-11T10:22:59-04:00","relpermalink":"/publication/cvpr_2021_semanticgan/","section":"publication","summary":"Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semisupervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces.","tags":[],"title":"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization","type":"publication"},{"authors":["He Wang","Yezhen Cong","Or Litany","Yue Gao","Leonidas J. Guibas"],"categories":null,"content":"","date":1618150919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618150919,"objectID":"53a2448442c8c7f2eddf7901c279444a","permalink":"/publication/cvpr_2021_3dioumatch/","publishdate":"2021-04-11T10:21:59-04:00","relpermalink":"/publication/cvpr_2021_3dioumatch/","section":"publication","summary":"3D object detection is an important yet demanding task that heavily relies on difficult to obtain 3D annotations. To reduce the required amount of supervision, we propose 3DIoUMatch, a novel semi-supervised method for 3D object detection applicable to both indoor and outdoor scenes. We leverage a teacher-student mutual learning framework to propagate information from the labeled to the unlabeled train set in the form of pseudo-labels. However, due to the high task complexity, we observe that the pseudo-labels suffer from significant noise and are thus not directly usable. To that end, we introduce a confidence-based filtering mechanism, inspired by FixMatch. We set confidence thresholds based upon the predicted objectness and class probability to filter low-quality pseudo-labels. While effective, we observe that these two measures do not sufficiently capture localization quality. We therefore propose to use the estimated 3D IoU as a localization metric and set category-aware self-adjusted thresholds to filter poorly localized proposals. We adopt VoteNet as our backbone detector on indoor datasets while we use PV-RCNN on the autonomous driving dataset, KITTI. Our method consistently improves state-of-the-art methods on both ScanNet and SUN-RGBD benchmarks by significant margins under all label ratios (including fully labeled setting). For example, when training using only 10% labeled data on ScanNet, 3DIoUMatch achieves 7.7 absolute improvement on mAP@0.25 and 8.5 absolute improvement on mAP@0.5 upon the prior art. On KITTI, we are the first to demonstrate semi-supervised 3D object detection and our method surpasses a fully supervised baseline from 1.8% to 7.6% under different label ratios and categories.","tags":[],"title":"3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection","type":"publication"},{"authors":["Yuxuan Zhang","Wenzheng Chen","Huan Ling","Jun Gao","Yinan Zhang","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1613226179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226179,"objectID":"0a362780443ad68776ee93144d75ce70","permalink":"/publication/iclr_2021_ganverse3d/","publishdate":"2021-02-13T10:22:59-04:00","relpermalink":"/publication/iclr_2021_ganverse3d/","section":"publication","summary":"Differentiable rendering has paved the way to training neural networks to perform 'inverse graphics' tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D 'neural renderer', complementing traditional graphics renderers.","tags":[],"title":"Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering","type":"publication"},{"authors":["Zhisheng Xiao","Karsten Kreis","Jan Kautz","Arash Vahdat"],"categories":null,"content":"","date":1613226178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226178,"objectID":"129cb72897c577af23deef94768a9736","permalink":"/publication/iclr_2021_-vaebm/","publishdate":"2021-02-13T10:22:58-04:00","relpermalink":"/publication/iclr_2021_-vaebm/","section":"publication","summary":"Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256×256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection.","tags":[],"title":"VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models","type":"publication"},{"authors":["Krishna Murthy Jatavallabhula","Miles Macklin","Florian Golemo","Vikram Voleti","Linda Petrini","Martin Weiss","Breandan Considine","Jérôme Parent-Lévesque","Kevin Xie","Kenny Erleben","Liam Paull","Florian Shkurti","Derek Nowrouzezahrai","Sanja Fidler"],"categories":null,"content":"","date":1613226177,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226177,"objectID":"40b530ba507f7dd3b30c1ec1a3422827","permalink":"/publication/iclr_2021_gradsim/","publishdate":"2021-02-13T10:22:57-04:00","relpermalink":"/publication/iclr_2021_gradsim/","section":"publication","summary":"We consider the problem of estimating an object’s physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present gradSim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph — spanning from the dynamics and through the rendering process — enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.","tags":[],"title":"gradSim: Differentiable simulation for system identification and visuomotor control","type":"publication"},{"authors":["Fei Xia","Chengshu Li","Roberto Martin-Martin","Alexander Toshev","Or Litany","Silvio Savarese"],"categories":null,"content":"","date":1610720519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610720519,"objectID":"2ca38ae9b97a3e641bed54663330f3cc","permalink":"/publication/icra_2021_relmogen/","publishdate":"2021-01-15T10:21:59-04:00","relpermalink":"/publication/icra_2021_relmogen/","section":"publication","summary":"Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator (a combination of motion planner and trajectory executor). We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen -- a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and Hierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots.","tags":[],"title":"ReLMoGen: Integrating Reinforcement Learning and Motion Generation for Interactive Navigation","type":"publication"},{"authors":["Tingwu Wang","Yunrong Guo","Maria Shugrina","Sanja Fidler"],"categories":["Computer Graphics","Computer Vision","Machine Learning","Robotics"],"content":"","date":1602739987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602739987,"objectID":"ad763fd0d6c31a497cf9762fb640649c","permalink":"/publication/unicon/","publishdate":"2020-10-15T01:33:07-04:00","relpermalink":"/publication/unicon/","section":"publication","summary":"The field of physics-based animation is gaining importance due to the increasing demand for realism in video games and films, and has recently seen wide adoption of data-driven techniques, such as deep reinforcement learning (RL), which learn control from (human) demonstrations. While RL has shown impressive results at reproducing individual motions and interactive locomotion, existing methods are limited in their ability to generalize to new motions and their ability to compose a complex motion sequence interactively. In this paper, we propose a physics-based universal neural controller (UniCon) that learns to master thousands of motions with different styles by learning on large-scale motion datasets. UniCon is a two-level framework that consists of a high-level motion scheduler and an RL-powered low-level motion executor, which is our key innovation. By systematically analyzing existing multi-motion RL frameworks, we introduce a novel objective function and training techniques which make a significant leap in performance. Once trained, our motion executor can be combined with different high-level schedulers without the need to retrain, enabling a variety of real-time interactive applications. We show that UniCon can support keyboard-driven control, compose motion sequences drawn from a large pool of locomotion and acrobatics skills and teleport a person captured on video to a physics-based virtual avatar. Numerical and qualitative results demonstrate a significant improvement in efficiency, robustness and generalizability of UniCon over prior state-of-the-art.","tags":["Computer Graphics"],"title":"UniCon: Universal Neural Controller For Physics-based Character Motion","type":"publication"},{"authors":["Krishna Murthy Jatavallabhula","Edward Smith","Jean-Francois Lafleche","Clement Fuji Tsang","Artem Rozantsev","Wenzheng Chen","Tommy Xiang","Rev Lebaredian","Sanja Fidler"],"categories":["Computer Vision"],"content":"News blogpost\nKaolin library documentation\nOmniverse Kaolin app documentation\n","date":1602653587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602653587,"objectID":"b3431fd68aebefba270a45187e318306","permalink":"/publication/kaolin/","publishdate":"2020-10-14T01:33:07-04:00","relpermalink":"/publication/kaolin/","section":"publication","summary":"Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.","tags":["Computer Vision"],"title":"Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research","type":"publication"},{"authors":["Huan Ling","David Acuna","Karsten Kreis","Seung Wook Kim","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1602567187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602567187,"objectID":"f10f6337abaa595062f149d31d2febe6","permalink":"/publication/variational_amodal_object_completion/","publishdate":"2020-10-13T01:33:07-04:00","relpermalink":"/publication/variational_amodal_object_completion/","section":"publication","summary":"","tags":["Computer Vision"],"title":"Variational Amodal Object Completion","type":"publication"},{"authors":["Jun Gao","Wenzheng Chen","Tommy Xiang","Alec Jacobson","Morgan Mcguire","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1601651361,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601651361,"objectID":"95d30d83e35d40520dc38cb60fc1b3a2","permalink":"/publication/def-tet/","publishdate":"2020-10-02T11:09:21-04:00","relpermalink":"/publication/def-tet/","section":"publication","summary":"3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce \u001bmph{Deformable Tetrahedral Meshes} (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTetmatches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D results using only a single image as input.","tags":["Computer Vision","Machine Learning"],"title":"Learning Deformable Tetrahedral Meshes for 3D Reconstruction","type":"publication"},{"authors":["Marc T. Law","Jos Stam"],"categories":["Machine Learning"],"content":"","date":1601616787,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601616787,"objectID":"7831776d5ecda2e923cf15ade5fac0c6","permalink":"/publication/ultrahyperbolic-representation-learning/","publishdate":"2020-10-02T01:33:07-04:00","relpermalink":"/publication/ultrahyperbolic-representation-learning/","section":"publication","summary":"In machine learning, data is usually represented in a (flat) Euclidean space where distances between points are along straight lines. Researchers have recently considered more exotic (non-Euclidean) Riemannian manifolds such as hyperbolic space which is well suited for tree-like data. In this paper, we propose a representation living on a pseudo-Riemannian manifold with constant nonzero curvature. It is a generalization of hyperbolic and spherical geometries where the nondegenerate metric tensor is not positive definite. We provide the necessary learning tools in this geometry and extend gradient method optimization techniques. More specifically, we provide closed-form expressions for distances via geodesics and define a descent direction that guarantees the minimization of the objective problem. Our novel framework is applied to graph representations.","tags":["Machine Learning","Differential Geometry","Optimization"],"title":"Ultrahyperbolic Representation Learning","type":"publication"},{"authors":["Daiqing Li","Amlan Kar","Nishant Ravikumar","Alejandro Frangi","Sanja Fidler"],"categories":["Medical Imaging"],"content":"","date":1601564321,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601564321,"objectID":"33e3d090209d0011de8f815e6b607c53","permalink":"/publication/fed-sim/","publishdate":"2020-10-01T10:58:41-04:00","relpermalink":"/publication/fed-sim/","section":"publication","summary":"Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets.","tags":["Medical Imaging"],"title":"Federated Simulation or Medical Imaging","type":"publication"},{"authors":["Jeevan Devaranjan*","Amlan Kar*","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599283987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599283987,"objectID":"edc9db344e59e5abb984cc595d460904","permalink":"/publication/metasim2/","publishdate":"2020-09-02T01:33:07-04:00","relpermalink":"/publication/metasim2/","section":"publication","summary":"Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods.","tags":["Computer Vision"],"title":"Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation","type":"publication"},{"authors":["Jonah Philion","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599197587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599197587,"objectID":"49eb27543bdebe2edc2645de000f4330","permalink":"/publication/lift-splat-shoot/","publishdate":"2020-09-03T01:33:07-04:00","relpermalink":"/publication/lift-splat-shoot/","section":"publication","summary":"The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single 'bird's-eye-view' coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to 'lift' each image individually into a frustum of features for each camera, then 'splat' all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by 'shooting' template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.","tags":["Computer Vision"],"title":"Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D","type":"publication"},{"authors":["Seung Wook Kim","Yuhao Zhou","Jonah Philion","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1590157319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590157319,"objectID":"b2a8d02ddf839b8af353c1b0940c132c","permalink":"/publication/gamegan/","publishdate":"2020-05-22T10:21:59-04:00","relpermalink":"/publication/gamegan/","section":"publication","summary":"Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN renders the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.","tags":[],"title":"Learning to Simulate Dynamic Environments with GameGAN","type":"publication"},{"authors":["Wenzheng Chen","Jun Gao","Huan Ling","Edward J. Smith","Jaakko Lehtinen","Alec Jacobson","Sanja Fidler"],"categories":null,"content":"","date":1565965319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565965319,"objectID":"655d147143897cd46301c6af3443f28a","permalink":"/publication/dib-r/","publishdate":"2019-08-16T10:21:59-04:00","relpermalink":"/publication/dib-r/","section":"publication","summary":"Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as an distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision.","tags":[],"title":"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer","type":"publication"},{"authors":["Hang Chu","Daiqing Li","David Acuna","Amlan Kar","Maria Shugrina","Xinkai Wei","Ming-Yu Liu","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1564582919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564582919,"objectID":"2eeb7b124f981358ef40bcc65e70b304","permalink":"/publication/ntg/","publishdate":"2019-07-31T10:21:59-04:00","relpermalink":"/publication/ntg/","section":"publication","summary":"We propose Neural Turtle Graphics (NTG), a novel gen- erative model for spatial graphs, and demonstrate its ap- plications in modeling city road layouts. Specifically, we represent the city road layout using a graph where nodes in the graph represent control points and edges in the graph represents segment of roads. NTG is a sequential genera- tive model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train the NTG model on Open Street Map data and show it outperforms ex- isting generative models using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.","tags":[],"title":"Neural Turtle Graphics for Modeling City Road Layouts","type":"publication"},{"authors":["Amlan Kar","Aayush Prakash","Ming-Yu Liu","Eric Cameracci","Justin Yuan","Matt Rusiniak","David Acuna","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1563632519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563632519,"objectID":"4a93c65409127e44851b288b5d8f0278","permalink":"/publication/meta_sim/","publishdate":"2019-07-20T10:21:59-04:00","relpermalink":"/publication/meta_sim/","section":"publication","summary":"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.","tags":[],"title":"Meta Sim: Learning to Generate Synthetic Datasets","type":"publication"},{"authors":["Towaki Takikawa","David Acuna","Varun Jampani","Sanja Fidler"],"categories":null,"content":"","date":1563286919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563286919,"objectID":"bf4d1e11ebe1cb7972e3c1a36197e8c7","permalink":"/publication/gscnn/","publishdate":"2019-07-16T10:21:59-04:00","relpermalink":"/publication/gscnn/","section":"publication","summary":"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. We propose a new architecture that adds a shape stream to the classical CNN architecture. The two streams process the image in parallel, and their information gets fused in the very top layers. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.","tags":[],"title":"Gated-SCNN Gated Shape CNNs for Semantic Segmentation","type":"publication"},{"authors":["David Acuna","Amlan Kar","Sanja Fidler"],"categories":null,"content":"","date":1555424519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555424519,"objectID":"413b78607950e3e5d3b8fd511b19539c","permalink":"/publication/steal/","publishdate":"2019-04-16T10:21:59-04:00","relpermalink":"/publication/steal/","section":"publication","summary":"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.","tags":[],"title":"Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations","type":"publication"}]
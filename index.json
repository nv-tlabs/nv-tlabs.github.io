[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://nv-tlabs.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Nelson Bighetti","type":"author"},{"authors":["Hang Chu","Daiqing Li","David Acuna","Amlan Kar","Maria Shugrina","Xinkai Wei","Ming-Yu Liu","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1564582919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564582919,"objectID":"2eeb7b124f981358ef40bcc65e70b304","permalink":"https://nv-tlabs.github.io/publication/ntg/","publishdate":"2019-07-31T10:21:59-04:00","relpermalink":"/publication/ntg/","section":"publication","summary":"We propose Neural Turtle Graphics (NTG), a novel gen- erative model for spatial graphs, and demonstrate its ap- plications in modeling city road layouts. Specifically, we represent the city road layout using a graph where nodes in the graph represent control points and edges in the graph represents segment of roads. NTG is a sequential genera- tive model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train the NTG model on Open Street Map data and show it outperforms ex- isting generative models using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.","tags":[],"title":"Neural Turtle Graphics for Modeling City Road Layouts","type":"publication"},{"authors":["Amlan Kar","Aayush Prakash","Ming-Yu Liu","Eric Cameracci","Justin Yuan","Matt Rusiniak","David Acuna","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1563632519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563632519,"objectID":"4a93c65409127e44851b288b5d8f0278","permalink":"https://nv-tlabs.github.io/publication/meta_sim/","publishdate":"2019-07-20T10:21:59-04:00","relpermalink":"/publication/meta_sim/","section":"publication","summary":"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.","tags":[],"title":"Meta Sim: Learning to Generate Synthetic Datasets","type":"publication"},{"authors":["Towaki Takikawa","David Acuna","Varun Jampani","Sanja Fidler"],"categories":null,"content":"","date":1563286919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563286919,"objectID":"bf4d1e11ebe1cb7972e3c1a36197e8c7","permalink":"https://nv-tlabs.github.io/publication/gscnn/","publishdate":"2019-07-16T10:21:59-04:00","relpermalink":"/publication/gscnn/","section":"publication","summary":"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. We propose a new architecture that adds a shape stream to the classical CNN architecture. The two streams process the image in parallel, and their information gets fused in the very top layers. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.","tags":[],"title":"Gated-SCNN Gated Shape CNNs for Semantic Segmentation","type":"publication"},{"authors":["David Acuna","Amlan Kar","Sanja Fidler"],"categories":null,"content":"","date":1555424519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555424519,"objectID":"413b78607950e3e5d3b8fd511b19539c","permalink":"https://nv-tlabs.github.io/publication/steal/","publishdate":"2019-04-16T10:21:59-04:00","relpermalink":"/publication/steal/","section":"publication","summary":"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.","tags":[],"title":"Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations","type":"publication"}]
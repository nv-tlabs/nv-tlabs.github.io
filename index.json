[{"authors":["sanjafidler"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a008223156da402f6f035b4e37e1bc13","permalink":"/author/sanja-fidler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sanja-fidler/","section":"authors","summary":"","tags":null,"title":"Sanja Fidler","type":"authors"},{"authors":["davidacunamarrero"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"052a025ce9438f4d0241f93c9d69c255","permalink":"/author/david-acuna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-acuna/","section":"authors","summary":"","tags":null,"title":"David Acuna","type":"authors"},{"authors":["tianshicao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"22fab2016a26de835b2c60c65dd3c6ce","permalink":"/author/tianshi-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianshi-cao/","section":"authors","summary":"","tags":null,"title":"Tianshi Cao","type":"authors"},{"authors":["bowenchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"46eb5d964c2c33917c1f46d49f219e44","permalink":"/author/bowen-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bowen-chen/","section":"authors","summary":"","tags":null,"title":"Bowen Chen","type":"authors"},{"authors":["wenzhengcheng"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d4aab82c8b2f97308fd9b03061baa62","permalink":"/author/wenzheng-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenzheng-chen/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Wenzheng Chen","type":"authors"},{"authors":["clementfujitsang"],"categories":null,"content":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c2429b6ac12c5dc84df0223e7517c65","permalink":"/author/clement-fuji-tsang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/clement-fuji-tsang/","section":"authors","summary":"I\u0026rsquo;m a research scientist at NVIDIA, leading Kaolin development and working on Deep Learning applied to 3D and computer vision.\nMy main focus is to develop and share Deep Learning solutions that are efficient and scalable on GPUs for 3D, computer vision and NLP tasks.","tags":null,"title":"Clement Fuji Tsang","type":"authors"},{"authors":["jungao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4290fc14b9975f84c3d7f082f170356","permalink":"/author/jun-gao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jun-gao/","section":"authors","summary":"","tags":null,"title":"Jun Gao","type":"authors"},{"authors":["amlankar"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cc1152b762fdb5eb7959a8087610fe02","permalink":"/author/amlan-kar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amlan-kar/","section":"authors","summary":"","tags":null,"title":"Amlan Kar","type":"authors"},{"authors":["samehkhamis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"732c7bd8ae9bcb1d929f34f6620da459","permalink":"/author/sameh-khamis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sameh-khamis/","section":"authors","summary":"","tags":null,"title":"Sameh Khamis","type":"authors"},{"authors":["seungwookkim"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"594015d730259c3a92f033305a532e2f","permalink":"/author/seung-wook-kim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/seung-wook-kim/","section":"authors","summary":"","tags":null,"title":"Seung Wook Kim","type":"authors"},{"authors":["karstenkreis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a216c907a09258df8f398909408d130a","permalink":"/author/karsten-kreis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/karsten-kreis/","section":"authors","summary":"","tags":null,"title":"Karsten Kreis","type":"authors"},{"authors":["marclaw"],"categories":null,"content":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks. More specifically, my domains of interest are: distance metric learning, complex representations including hierarchies and graphs, convex and non-convex optimization, optimization on manifolds, information geometry, structured output prediction, generalization with limited supervision.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f02ed7ceb48db52a1c5142bb4607614b","permalink":"/author/marc-law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marc-law/","section":"authors","summary":"I am a senior research scientist at NVIDIA working on machine learning and computer vision.\nMy main focus is to propose scalable machine learning methods that can be applied to computer vision tasks.","tags":null,"title":"Marc Law","type":"authors"},{"authors":["daiqingli"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ee0d6dbbf827a332ae71729722511b7a","permalink":"/author/daiqing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daiqing-li/","section":"authors","summary":"","tags":null,"title":"Daiqing Li","type":"authors"},{"authors":["huanling"],"categories":null,"content":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year. My advisor is Prof. Sanja Fidler.\nWebsite\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eb3fc86e8c4373b23194a1d7a96d5481","permalink":"/author/huan-ling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huan-ling/","section":"authors","summary":"I am a Research Scientist at NVIDIA Toronto AI Lab. I am also a grad student at University of Toronto, machine learning group and Vector Institute. I am currently in my GAP year.","tags":null,"title":"Huan Ling","type":"authors"},{"authors":["joeylitalien"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"58cac3c6accbb4f342c3d19de02b3e2c","permalink":"/author/joey-litalien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/joey-litalien/","section":"authors","summary":"","tags":null,"title":"Joey Litalien","type":"authors"},{"authors":["orlitany"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"446ebc3097a2014a6dac95a5dcfe857c","permalink":"/author/or-litany/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/or-litany/","section":"authors","summary":"","tags":null,"title":"Or Litany","type":"authors"},{"authors":["rafidmahmood"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b4ba619dd5ed502941db478c875271c","permalink":"/author/rafid-mahmood/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rafid-mahmood/","section":"authors","summary":"","tags":null,"title":"Rafid Mahmood","type":"authors"},{"authors":["despoinapaschalidou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"71062a171cd9dcbb0030b96d536ffd19","permalink":"/author/despoina-paschalidou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/despoina-paschalidou/","section":"authors","summary":"","tags":null,"title":"Despoina Paschalidou","type":"authors"},{"authors":["jonahphilion"],"categories":null,"content":"Website\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d881c29c284c9572f14b5f49c6c6d13e","permalink":"/author/jonah-philion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonah-philion/","section":"authors","summary":"Website","tags":null,"title":"Jonah Philion","type":"authors"},{"authors":["cinjonresnick"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7004af2e6a5b0c3f3b2131552c2cc1b8","permalink":"/author/cinjon-resnick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cinjon-resnick/","section":"authors","summary":"","tags":null,"title":"Cinjon Resnick","type":"authors"},{"authors":["frankshen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3dd516552086188aec3e991458f3bf5","permalink":"/author/frank-shen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/frank-shen/","section":"authors","summary":"","tags":null,"title":"Frank Shen","type":"authors"},{"authors":["mashashugrina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b710e53d7696c37bd5f9e1c5b7f4147","permalink":"/author/masha-shugrina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/masha-shugrina/","section":"authors","summary":"","tags":null,"title":"Masha Shugrina","type":"authors"},{"authors":["zianwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ff622ef7205d9df98598e14b66b9609","permalink":"/author/zian-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zian-wang/","section":"authors","summary":"","tags":null,"title":"Zian Wang","type":"authors"},{"authors":["towakitakikawa"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ce5d6218d34a263e6c634c625045dfa4","permalink":"/author/towaki-takikawa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/towaki-takikawa/","section":"authors","summary":"","tags":null,"title":"Towaki Takikawa","type":"authors"},{"authors":["tommyxiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ba5ce1113200696d0ce4a55f1555f978","permalink":"/author/tommy-xiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tommy-xiang/","section":"authors","summary":"","tags":null,"title":"Tommy Xiang","type":"authors"},{"authors":["kevinxie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b273d4708ae30cdc04370ea1a5d7ace7","permalink":"/author/kevin-xie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kevin-xie/","section":"authors","summary":"","tags":null,"title":"Kevin Xie","type":"authors"},{"authors":["junlinyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d45085dc6bc342fd9b9bcd6ee2ed7c62","permalink":"/author/junlin-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junlin-yang/","section":"authors","summary":"","tags":null,"title":"Junlin Yang","type":"authors"},{"authors":["kangxueyin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"32703f897bd8424fc595e7af09cf9d14","permalink":"/author/kangxue-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kangxue-yin/","section":"authors","summary":"","tags":null,"title":"Kangxue Yin","type":"authors"},{"authors":["alexzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"df402183d6954aec8a0979fda4aac6ab","permalink":"/author/alex-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-zhang/","section":"authors","summary":"","tags":null,"title":"Alex Zhang","type":"authors"},{"authors":["siyanzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0721bfdb7795b0de16a0ba459c4719ae","permalink":"/author/siyan-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siyan-zhao/","section":"authors","summary":"","tags":null,"title":"Siyan Zhao","type":"authors"},{"authors":["tingwuwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"00033e991e190cf6e4a354975f38e86d","permalink":"/author/tingwu-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tingwu-wang/","section":"authors","summary":"","tags":null,"title":"Tingwu Wang","type":"authors"},{"authors":["adityakusupati"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b61a279523f54138e79b739ad678a79","permalink":"/author/aditya-kusupati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aditya-kusupati/","section":"authors","summary":"","tags":null,"title":"Aditya Kusupati","type":"authors"},{"authors":["alexbie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fcb08531a5bf13e454f0ff379c929ea9","permalink":"/author/alex-bie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-bie/","section":"authors","summary":"","tags":null,"title":"Alex Bie","type":"authors"},{"authors":["guojunzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"97d79e79ff77e9981453b8f24aba6fcf","permalink":"/author/guojun-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guojun-zhang/","section":"authors","summary":"","tags":null,"title":"Guojun Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics. Our group members are also part of or closely collaborate with academic labs such as the University of Toronto and Vector Institute.\nWe invite applications for the following positions:\n full time research scientist full time research engineer research scientist intern research engineer intern  See this link for open positions, or contact our members for more details.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/toronto-ai-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/toronto-ai-lab/","section":"authors","summary":"Welcome to the homepage of the NVIDIA Toronto Artificial Intelligence Lab led by Professor Sanja Fidler. Our research group was founded in 2018, and is primarily based in Toronto.\nThe research interests of our lab lie at the intersection of computer vision, machine learning and computer graphics.","tags":null,"title":"Toronto AI Lab","type":"authors"},{"authors":["Yuxuan Zhang","Huan Ling","Jun Gao","Kangxue Yin","Jean-Francois Lafleche","Adela Barriuso","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1618496519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618496519,"objectID":"945e62e056e2517eef12fae1051ad8f1","permalink":"/publication/datasetgan/","publishdate":"2021-04-15T10:21:59-04:00","relpermalink":"/publication/datasetgan/","section":"publication","summary":"We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort. Current deep networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, resulting in an infinite annotated dataset generator! These generated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines significantly and is on par with fully supervised methods using labor intensive annotations.","tags":[],"title":"DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort","type":"publication"},{"authors":["Towaki Takikawa","Joey Litalien","Kangxue Yin","Karsten Kreis","Charles Loop","Derek Nowrouzezahrai","Alec Jacobson","Morgan McGuire","Sanja Fidler"],"categories":null,"content":"","date":1611584519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611584519,"objectID":"39b15f7e2f8ea9430aca897034f5d0f7","permalink":"/publication/nglod/","publishdate":"2021-01-25T10:21:59-04:00","relpermalink":"/publication/nglod/","section":"publication","summary":" Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics. ","tags":[],"title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Surfaces","type":"publication"},{"authors":["Tingwu Wang","Yunrong Guo","Maria Shugrina","Sanja Fidler"],"categories":["Computer Graphics","Computer Vision","Machine Learning","Robotics"],"content":"","date":1602739987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602739987,"objectID":"ad763fd0d6c31a497cf9762fb640649c","permalink":"/publication/unicon/","publishdate":"2020-10-15T01:33:07-04:00","relpermalink":"/publication/unicon/","section":"publication","summary":"The field of physics-based animation is gaining importance due to the increasing demand for realism in video games and films, and has recently seen wide adoption of data-driven techniques, such as deep reinforcement learning (RL), which learn control from (human) demonstrations. While RL has shown impressive results at reproducing individual motions and interactive locomotion, existing methods are limited in their ability to generalize to new motions and their ability to compose a complex motion sequence interactively. In this paper, we propose a physics-based universal neural controller (UniCon) that learns to master thousands of motions with different styles by learning on large-scale motion datasets. UniCon is a two-level framework that consists of a high-level motion scheduler and an RL-powered low-level motion executor, which is our key innovation. By systematically analyzing existing multi-motion RL frameworks, we introduce a novel objective function and training techniques which make a significant leap in performance. Once trained, our motion executor can be combined with different high-level schedulers without the need to retrain, enabling a variety of real-time interactive applications. We show that UniCon can support keyboard-driven control, compose motion sequences drawn from a large pool of locomotion and acrobatics skills and teleport a person captured on video to a physics-based virtual avatar. Numerical and qualitative results demonstrate a significant improvement in efficiency, robustness and generalizability of UniCon over prior state-of-the-art.","tags":["Computer Graphics"],"title":"UniCon: Universal Neural Controller For Physics-based Character Motion","type":"publication"},{"authors":["Krishna Murthy Jatavallabhula","Edward Smith","Jean-Francois Lafleche","Clement Fuji Tsang","Artem Rozantsev","Wenzheng Chen","Tommy Xiang","Rev Lebaredian","Sanja Fidler"],"categories":["Computer Vision"],"content":"News blogpost\nKaolin library documentation\nOmniverse Kaolin app documentation\n","date":1602653587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602653587,"objectID":"b3431fd68aebefba270a45187e318306","permalink":"/publication/kaolin/","publishdate":"2020-10-14T01:33:07-04:00","relpermalink":"/publication/kaolin/","section":"publication","summary":"Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.","tags":["Computer Vision"],"title":"Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research","type":"publication"},{"authors":["Huan Ling","David Acuna","Karsten Kreis","Seung Wook Kim","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1602567187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602567187,"objectID":"f10f6337abaa595062f149d31d2febe6","permalink":"/publication/variational_amodal_object_completion/","publishdate":"2020-10-13T01:33:07-04:00","relpermalink":"/publication/variational_amodal_object_completion/","section":"publication","summary":"","tags":["Computer Vision"],"title":"Variational Amodal Object Completion","type":"publication"},{"authors":["Jun Gao","Wenzheng Chen","Tommy Xiang","Alec Jacobson","Morgan Mcguire","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1601651361,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601651361,"objectID":"95d30d83e35d40520dc38cb60fc1b3a2","permalink":"/publication/def-tet/","publishdate":"2020-10-02T11:09:21-04:00","relpermalink":"/publication/def-tet/","section":"publication","summary":"3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce \u001bmph{Deformable Tetrahedral Meshes} (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTetmatches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D results using only a single image as input.","tags":["Computer Vision","Machine Learning"],"title":"Learning Deformable Tetrahedral Meshes for 3D Reconstruction","type":"publication"},{"authors":["Marc T. Law","Jos Stam"],"categories":["Machine Learning"],"content":"","date":1601616787,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601616787,"objectID":"7831776d5ecda2e923cf15ade5fac0c6","permalink":"/publication/ultrahyperbolic-representation-learning/","publishdate":"2020-10-02T01:33:07-04:00","relpermalink":"/publication/ultrahyperbolic-representation-learning/","section":"publication","summary":"In machine learning, data is usually represented in a (flat) Euclidean space where distances between points are along straight lines. Researchers have recently considered more exotic (non-Euclidean) Riemannian manifolds such as hyperbolic space which is well suited for tree-like data. In this paper, we propose a representation living on a pseudo-Riemannian manifold with constant nonzero curvature. It is a generalization of hyperbolic and spherical geometries where the nondegenerate metric tensor is not positive definite. We provide the necessary learning tools in this geometry and extend gradient method optimization techniques. More specifically, we provide closed-form expressions for distances via geodesics and define a descent direction that guarantees the minimization of the objective problem. Our novel framework is applied to graph representations.","tags":["Machine Learning","Differential Geometry","Optimization"],"title":"Ultrahyperbolic Representation Learning","type":"publication"},{"authors":["Daiqing Li","Amlan Kar","Nishant Ravikumar","Alejandro Frangi","Sanja Fidler"],"categories":["Medical Imaging"],"content":"","date":1601564321,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601564321,"objectID":"33e3d090209d0011de8f815e6b607c53","permalink":"/publication/fed-sim/","publishdate":"2020-10-01T10:58:41-04:00","relpermalink":"/publication/fed-sim/","section":"publication","summary":"Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets.","tags":["Medical Imaging"],"title":"Federated Simulation or Medical Imaging","type":"publication"},{"authors":["Jeevan Devaranjan*","Amlan Kar*","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599283987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599283987,"objectID":"edc9db344e59e5abb984cc595d460904","permalink":"/publication/metasim2/","publishdate":"2020-09-02T01:33:07-04:00","relpermalink":"/publication/metasim2/","section":"publication","summary":"Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods.","tags":["Computer Vision"],"title":"Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation","type":"publication"},{"authors":["Jonah Philion","Sanja Fidler"],"categories":["Computer Vision"],"content":"","date":1599197587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599197587,"objectID":"49eb27543bdebe2edc2645de000f4330","permalink":"/publication/lift-splat-shoot/","publishdate":"2020-09-03T01:33:07-04:00","relpermalink":"/publication/lift-splat-shoot/","section":"publication","summary":"The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single 'bird's-eye-view' coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to 'lift' each image individually into a frustum of features for each camera, then 'splat' all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by 'shooting' template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.","tags":["Computer Vision"],"title":"Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D","type":"publication"},{"authors":["Seung Wook Kim","Yuhao Zhou","Jonah Philion","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1590157319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590157319,"objectID":"b2a8d02ddf839b8af353c1b0940c132c","permalink":"/publication/gamegan/","publishdate":"2020-05-22T10:21:59-04:00","relpermalink":"/publication/gamegan/","section":"publication","summary":"Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN renders the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.","tags":[],"title":"Learning to Simulate Dynamic Environments with GameGAN","type":"publication"},{"authors":["Wenzheng Chen","Jun Gao","Huan Ling","Edward J. Smith","Jaakko Lehtinen","Alec Jacobson","Sanja Fidler"],"categories":null,"content":"","date":1565965319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565965319,"objectID":"655d147143897cd46301c6af3443f28a","permalink":"/publication/dib-r/","publishdate":"2019-08-16T10:21:59-04:00","relpermalink":"/publication/dib-r/","section":"publication","summary":"Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as an distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision.","tags":[],"title":"Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer","type":"publication"},{"authors":["Hang Chu","Daiqing Li","David Acuna","Amlan Kar","Maria Shugrina","Xinkai Wei","Ming-Yu Liu","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1564582919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564582919,"objectID":"2eeb7b124f981358ef40bcc65e70b304","permalink":"/publication/ntg/","publishdate":"2019-07-31T10:21:59-04:00","relpermalink":"/publication/ntg/","section":"publication","summary":"We propose Neural Turtle Graphics (NTG), a novel gen- erative model for spatial graphs, and demonstrate its ap- plications in modeling city road layouts. Specifically, we represent the city road layout using a graph where nodes in the graph represent control points and edges in the graph represents segment of roads. NTG is a sequential genera- tive model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train the NTG model on Open Street Map data and show it outperforms ex- isting generative models using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.","tags":[],"title":"Neural Turtle Graphics for Modeling City Road Layouts","type":"publication"},{"authors":["Amlan Kar","Aayush Prakash","Ming-Yu Liu","Eric Cameracci","Justin Yuan","Matt Rusiniak","David Acuna","Antonio Torralba","Sanja Fidler"],"categories":null,"content":"","date":1563632519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563632519,"objectID":"4a93c65409127e44851b288b5d8f0278","permalink":"/publication/meta_sim/","publishdate":"2019-07-20T10:21:59-04:00","relpermalink":"/publication/meta_sim/","section":"publication","summary":"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.","tags":[],"title":"Meta Sim: Learning to Generate Synthetic Datasets","type":"publication"},{"authors":["Towaki Takikawa","David Acuna","Varun Jampani","Sanja Fidler"],"categories":null,"content":"","date":1563286919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563286919,"objectID":"bf4d1e11ebe1cb7972e3c1a36197e8c7","permalink":"/publication/gscnn/","publishdate":"2019-07-16T10:21:59-04:00","relpermalink":"/publication/gscnn/","section":"publication","summary":"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. We propose a new architecture that adds a shape stream to the classical CNN architecture. The two streams process the image in parallel, and their information gets fused in the very top layers. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.","tags":[],"title":"Gated-SCNN Gated Shape CNNs for Semantic Segmentation","type":"publication"},{"authors":["David Acuna","Amlan Kar","Sanja Fidler"],"categories":null,"content":"","date":1555424519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555424519,"objectID":"413b78607950e3e5d3b8fd511b19539c","permalink":"/publication/steal/","publishdate":"2019-04-16T10:21:59-04:00","relpermalink":"/publication/steal/","section":"publication","summary":"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.","tags":[],"title":"Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations","type":"publication"}]